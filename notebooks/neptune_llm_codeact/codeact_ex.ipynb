{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# based on the math example in: https://github.com/langchain-ai/langgraph-codeact/blob/harrison/some-changes/examples/math_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Generated Response:\n",
      " <think>\n",
      "Okay, I need to create a Python function that generates the Fibonacci sequence. Hmm, what's the Fibonacci sequence again? Oh right, it starts with 0 and 1, and each subsequent number is the sum of the previous two. So like 0, 1, 1, 2, 3, 5, etc.\n",
      "\n",
      "First, I should decide how to make this function. Should it take a specific number of terms as an argument? That makes sense because then the user can specify how long they want the sequence. Let's say the function is called fibonacci and takes n as an input.\n",
      "\n",
      "What if someone calls the function with n=0 or 1? Well, for n=0, maybe return an empty list. For n=1, return [0]. For higher numbers, start building from there.\n",
      "\n",
      "So I'll initialize a list to hold the sequence. If n is 0, return an empty list. Else, add 0 and 1 as the first two elements if n is at least that big.\n",
      "\n",
      "Then, for each subsequent term up to n terms, calculate it by adding the last two numbers. Append each new number to the list until we reach the desired count.\n",
      "\n",
      "Wait, but what about when someone passes a negative number? Maybe handle that by returning an empty list or raising an error. Oh, perhaps just return an empty list for invalid inputs.\n",
      "\n",
      "Let me outline the steps:\n",
      "\n",
      "1. Check if n is less than 0: return []\n",
      "2. Initialize the sequence with the first two numbers, starting at index 0.\n",
      "3. Loop from the third term up to n terms:\n",
      "   a. Each new term is sum of previous two\n",
      "   b. Append it to the list\n",
      "4. Return the list\n",
      "\n",
      "Testing this logic:\n",
      "\n",
      "For example, if n=5, should get [0,1,1,2,3]\n",
      "\n",
      "Wait, no: 0,1, then 1 (0+1), 2 (1+1), 3 (1+2). So yes.\n",
      "\n",
      "Another test case: n=7 would be [0,1,1,2,3,5,8].\n",
      "\n",
      "What about edge cases? Like n=0 returns empty. n=1 returns [0]. That's handled in the initial steps.\n",
      "\n",
      "I think that should cover it. Now write this into code.\n",
      "</think>\n",
      "\n",
      "To generate a Fibonacci sequence generator function in Python:\n",
      "\n",
      "**Step-by-step Explanation:**\n",
      "\n",
      "1. **Define the Function:** Create a function named `fibonacci` that takes an integer `n` as its parameter, representing the number of terms to generate.\n",
      "\n",
      "2. **Handle Edge Cases:**\n",
      "   - If `n` is less than 0, return an empty list.\n",
      "   - If `n` is 0, return an empty list.\n",
      "   - If `n` is 1, return a list containing only `[0]`.\n",
      "\n",
      "3. **Initialize the Sequence:** Start with a list containing the first two Fibonacci numbers, which are `0` and `1`.\n",
      "\n",
      "4. **Generate Subsequent Terms:**\n",
      "   - Use a loop starting from index 2 up to `n-1`.\n",
      "   - For each iteration, calculate the next number as the sum of the previous two elements in the sequence.\n",
      "   - Append this new number to the sequence list.\n",
      "\n",
      "5. **Return the Sequence:** After generating all required terms, return the complete Fibonacci sequence list.\n",
      "\n",
      "**Python Code:**\n",
      "\n",
      "```python\n",
      "def fibonacci(n):\n",
      "    if n < 0:\n",
      "        return []\n",
      "    elif n == 0:\n",
      "        return []\n",
      "    else:\n",
      "        fib_sequence = [0, 1]\n",
      "        for i in range(2, n):\n",
      "            next_num = fib_sequence[i-1] + fib_sequence[i-2]\n",
      "            fib_sequence.append(next_num)\n",
      "        # If exactly two terms were requested\n",
      "        if n == 2:\n",
      "            return fib_sequence[:2]\n",
      "        else:\n",
      "            return fib_sequence\n",
      "\n",
      "# Example usage:\n",
      "print(fibonacci(5))   # Output: [0, 1, 1, 2, 3]\n",
      "print(fibonacci(7))   # Output: [0, 1, 1, 2, 3, 5, 8]\n",
      "```\n",
      "\n",
      "**Example Outputs:**\n",
      "\n",
      "- `fibonacci(5)` returns `[0, 1, 1, 2, 3]`\n",
      "- `fibonacci(7)` returns `[0, 1, 1, 2, 3, 5, 8]`\n",
      "\n",
      "[DEBUG] Extracted Code Representation:\n",
      "'def fibonacci(n):\\n    if n < 0:\\n        return []\\n    elif n == 0:\\n        return []\\n    else:\\n        fib_sequence = [0, 1]\\n        for i in range(2, n):\\n            next_num = fib_sequence[i-1] + fib_sequence[i-2]\\n            fib_sequence.append(next_num)\\n        # If exactly two terms were requested\\n        if n == 2:\\n            return fib_sequence[:2]\\n        else:\\n            return fib_sequence\\n\\n# Example usage:\\nprint(fibonacci(5))   # Output: [0, 1, 1, 2, 3]\\nprint(fibonacci(7))   # Output: [0, 1, 1, 2, 3, 5, 8]'\n",
      "\n",
      "[DEBUG] Complete Test Code:\n",
      "\"\\ndef fibonacci(n):\\n    if n < 0:\\n        return []\\n    elif n == 0:\\n        return []\\n    else:\\n        fib_sequence = [0, 1]\\n        for i in range(2, n):\\n            next_num = fib_sequence[i-1] + fib_sequence[i-2]\\n            fib_sequence.append(next_num)\\n        # If exactly two terms were requested\\n        if n == 2:\\n            return fib_sequence[:2]\\n        else:\\n            return fib_sequence\\n\\n# Example usage:\\nprint(fibonacci(5))   # Output: [0, 1, 1, 2, 3]\\nprint(fibonacci(7))   # Output: [0, 1, 1, 2, 3, 5, 8]\\n\\n# Example usage of generated function\\nif __name__ == '__main__':\\n    fib_sequence = fibonacci(10)\\n    print('Fibonacci sequence:', fib_sequence)\\n\"\n",
      "\n",
      "Sandbox Execution Output:\n",
      " [0, 1, 1, 2, 3]\n",
      "[0, 1, 1, 2, 3, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import builtins\n",
    "import contextlib\n",
    "import io\n",
    "from typing import Any\n",
    "import re\n",
    "import textwrap  # Ensure this is added to your imports\n",
    "import traceback  # Ensure this is imported at the top\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph_codeact import create_codeact\n",
    "\n",
    "\n",
    "def extract_python_code(response_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts and returns the Python code contained in a markdown code block from the response text.\n",
    "    Searches for a block delimited by '```python' and '```'. \n",
    "    Raises a ValueError if no valid Python code block is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```python\\n(.*?)\\n```\", response_text, re.DOTALL)\n",
    "    if match:\n",
    "        # Return only the code inside the markdown code block, stripped of extra whitespace.\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        raise ValueError(\"No valid Python code block found in the response.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Function: generate_python_function\n",
    "# ---------------------------\n",
    "def generate_python_function(description): \n",
    "    response = ollama.chat(model='deepseek-r1:latest', messages=[\n",
    "        {'role': 'user', 'content': f'Generate a Python function for: {description}'}\n",
    "    ])   \n",
    "    return response['message']['content']\n",
    "\n",
    "# ---------------------------\n",
    "# Function: sandbox_run\n",
    "# ---------------------------\n",
    "def sandbox_run(code: str, env: dict[str, Any]) -> tuple[str, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Execute code safely and capture stdout.\n",
    "    \"\"\"\n",
    "    _locals = env.copy()\n",
    "    original_keys = set(_locals.keys())\n",
    "    try:\n",
    "        f = io.StringIO()\n",
    "        with contextlib.redirect_stdout(f):\n",
    "            exec(code, {\"__builtins__\": builtins.__dict__}, _locals)\n",
    "        result_output = f.getvalue().strip() or \"<no output>\"\n",
    "    except Exception as e:\n",
    "        # Capture and include the full traceback for debugging purposes\n",
    "        result_output = f\"Error during execution: {e}\\n{traceback.format_exc()}\"\n",
    "    new_vars = {k: _locals[k] for k in _locals.keys() - original_keys}\n",
    "    return result_output, new_vars\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main execution to test integration\n",
    "# ---------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function to generate Python code, extract it, format correctly,\n",
    "    execute safely, and print outputs.\n",
    "    \"\"\"\n",
    "    # Generate Python function code using Ollama\n",
    "    description = 'fibonacci sequence generator'\n",
    "    raw_generated_response = generate_python_function(description)\n",
    "    print(\"Raw Generated Response:\\n\", raw_generated_response)\n",
    "    \n",
    "    # Extract only the Python code from the generated response\n",
    "    try:\n",
    "        generated_code = extract_python_code(raw_generated_response)\n",
    "    except ValueError as e:\n",
    "        print(\"Error extracting code:\", e)\n",
    "        return\n",
    "\n",
    "    # Debug: Print the raw representation of the extracted code to inspect whitespace and formatting\n",
    "    print(\"\\n[DEBUG] Extracted Code Representation:\")\n",
    "    print(repr(generated_code))\n",
    "    \n",
    "    # Attempt to compile the generated code to catch syntax errors early\n",
    "    try:\n",
    "        compile(generated_code, '<string>', 'exec')\n",
    "    except Exception as e:\n",
    "        print(\"Compilation error in generated code:\", e)\n",
    "        return\n",
    "\n",
    "    # Prepare the test code without disturbing the generated code's indentation\n",
    "    test_code = f\"\"\"\n",
    "{generated_code}\n",
    "\n",
    "# Example usage of generated function\n",
    "if __name__ == '__main__':\n",
    "    fib_sequence = fibonacci(10)\n",
    "    print('Fibonacci sequence:', fib_sequence)\n",
    "\"\"\"\n",
    "    # Debug: Print the complete test code for verification\n",
    "    print(\"\\n[DEBUG] Complete Test Code:\")\n",
    "    print(repr(test_code))\n",
    "\n",
    "    # Execute the test code safely using the sandbox\n",
    "    output, new_vars = sandbox_run(test_code, {})\n",
    "    print(\"\\nSandbox Execution Output:\\n\", output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---answer---\n",
      "\n",
      " {'messages': [HumanMessage(content='A batter hits a baseball at 45.847 m/s at an angle of 23.474° above the horizontal. The outfielder, who starts facing the batter, picks up the baseball as it lands, then throws it back towards the batter at 24.12 m/s at an angle of 39.12 degrees. How far is the baseball from where the batter originally hit it? Assume zero air resistance.', additional_kwargs={}, response_metadata={}, id='b40d87a4-2e4d-4a3a-b301-e5c4582b29b6')]}\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Message\nrole\n  Field required [type=missing, input_value={'content': 'A batter hit...4a3a-b301-e5c4582b29b6'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 170\u001b[0m\n\u001b[0;32m    157\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    158\u001b[0m     {\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m     }\n\u001b[0;32m    167\u001b[0m ]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Stream the agent's response and print the output\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m typ, chunk \u001b[38;5;129;01min\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m    171\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages},\n\u001b[0;32m    172\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    173\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}},\n\u001b[0;32m    174\u001b[0m ):\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;28mprint\u001b[39m(chunk[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\langgraph\\pregel\\__init__.py:2331\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2325\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   2326\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   2327\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   2328\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   2329\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   2330\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 2331\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2332\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   2333\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2334\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   2335\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2336\u001b[0m         ):\n\u001b[0;32m   2337\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2338\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   2339\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\langgraph\\pregel\\runner.py:232\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpanic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\langgraph\\pregel\\runner.py:458\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(futs, timeout_exc_cls, panic)\u001b[0m\n\u001b[0;32m    456\u001b[0m                 interrupts\u001b[38;5;241m.\u001b[39mappend(exc)\n\u001b[0;32m    457\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\langgraph\\pregel\\executor.py:83\u001b[0m, in \u001b[0;36mBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\langgraph\\utils\\runnable.py:606\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    603\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    604\u001b[0m )\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\langgraph\\utils\\runnable.py:371\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 371\u001b[0m         ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\langgraph_codeact\\__init__.py:77\u001b[0m, in \u001b[0;36mcreate_codeact.<locals>.call_model\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_model\u001b[39m(state: CodeActState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Command:\n\u001b[0;32m     76\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}] \u001b[38;5;241m+\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 77\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;66;03m# get content between fences\u001b[39;00m\n\u001b[0;32m     80\u001b[0m         code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[4], line 97\u001b[0m, in \u001b[0;36mOllamaChatModel.invoke\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m    Synchronously invoke the model by wrapping the streaming interface.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    This method calls `stream` and returns the first message object.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m typ, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     99\u001b[0m             \u001b[38;5;66;03m# Return the first message from the chunk\u001b[39;00m\n\u001b[0;32m    100\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m chunk[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[4], line 127\u001b[0m, in \u001b[0;36mOllamaChatModel.stream\u001b[1;34m(self, inputs, stream_mode, config)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type for stream; expected dict or list.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Call the Ollama API with the provided messages\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Create a simple message object with a 'content' attribute\u001b[39;00m\n\u001b[0;32m    129\u001b[0m message_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, (), {})()\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\ollama\\_client.py:339\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    334\u001b[0m     ChatResponse,\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    337\u001b[0m     json\u001b[38;5;241m=\u001b[39mChatRequest(\n\u001b[0;32m    338\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m--> 339\u001b[0m       messages\u001b[38;5;241m=\u001b[39m[message \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m _copy_messages(messages)],\n\u001b[0;32m    340\u001b[0m       tools\u001b[38;5;241m=\u001b[39m[tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m _copy_tools(tools)],\n\u001b[0;32m    341\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    342\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    343\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    344\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m    345\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    346\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    347\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\ollama\\_client.py:339\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(\n\u001b[0;32m    290\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    291\u001b[0m   model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    334\u001b[0m     ChatResponse,\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/api/chat\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    337\u001b[0m     json\u001b[38;5;241m=\u001b[39mChatRequest(\n\u001b[0;32m    338\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m--> 339\u001b[0m       messages\u001b[38;5;241m=\u001b[39m[message \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m _copy_messages(messages)],\n\u001b[0;32m    340\u001b[0m       tools\u001b[38;5;241m=\u001b[39m[tool \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m _copy_tools(tools)],\n\u001b[0;32m    341\u001b[0m       stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    342\u001b[0m       \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m,\n\u001b[0;32m    343\u001b[0m       options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    344\u001b[0m       keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m    345\u001b[0m     )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    346\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    347\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\ollama\\_client.py:1135\u001b[0m, in \u001b[0;36m_copy_messages\u001b[1;34m(messages)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_copy_messages\u001b[39m(messages: Optional[Sequence[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Message]]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Message]:\n\u001b[0;32m   1134\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages \u001b[38;5;129;01mor\u001b[39;00m []:\n\u001b[1;32m-> 1135\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m      \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ghadf\\anaconda3\\envs\\data_science_ollama\\lib\\site-packages\\pydantic\\main.py:627\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[1;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    626\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Message\nrole\n  Field required [type=missing, input_value={'content': 'A batter hit...4a3a-b301-e5c4582b29b6'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import builtins\n",
    "import contextlib\n",
    "import io\n",
    "import math\n",
    "from typing import Any\n",
    "from langgraph_codeact import create_codeact\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Existing eval function (unchanged)\n",
    "def eval(code: str, _locals: dict[str, Any]) -> tuple[str, dict[str, Any]]:\n",
    "    # Store original keys before execution\n",
    "    original_keys = set(_locals.keys())\n",
    "    try:\n",
    "        with contextlib.redirect_stdout(io.StringIO()) as f:\n",
    "            exec(code, builtins.__dict__, _locals)\n",
    "        result = f.getvalue()\n",
    "        if not result:\n",
    "            result = \"<code ran, no output printed to stdout>\"\n",
    "    except Exception as e:\n",
    "        result = f\"Error during execution: {repr(e)}\"\n",
    "    # Determine new variables created during execution\n",
    "    new_keys = set(_locals.keys()) - original_keys\n",
    "    new_vars = {key: _locals[key] for key in new_keys}\n",
    "    return result, new_vars\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Divide one number by another.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "def subtract(a: float, b: float) -> float:\n",
    "    \"\"\"Subtract one number from another.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "def sin(a: float) -> float:\n",
    "    \"\"\"Calculate the sine of a number (in radians).\"\"\"\n",
    "    return math.sin(a)\n",
    "\n",
    "def cos(a: float) -> float:\n",
    "    \"\"\"Calculate the cosine of a number (in radians).\"\"\"\n",
    "    return math.cos(a)\n",
    "\n",
    "def radians(a: float) -> float:\n",
    "    \"\"\"Convert degrees to radians.\"\"\"\n",
    "    return math.radians(a)\n",
    "\n",
    "def exponentiation(a: float, b: float) -> float:\n",
    "    \"\"\"Raise a number to the power of another.\"\"\"\n",
    "    return a ** b\n",
    "\n",
    "def sqrt(a: float) -> float:\n",
    "    \"\"\"Compute the square root of a number.\"\"\"\n",
    "    return math.sqrt(a)\n",
    "\n",
    "def ceil(a: float) -> float:\n",
    "    \"\"\"Round a number up to the nearest integer.\"\"\"\n",
    "    return math.ceil(a)\n",
    "\n",
    "\n",
    "# List of tool functions\n",
    "tools = [\n",
    "    add,\n",
    "    multiply,\n",
    "    divide,\n",
    "    subtract,\n",
    "    sin,\n",
    "    cos,\n",
    "    radians,\n",
    "    exponentiation,\n",
    "    sqrt,\n",
    "    ceil,\n",
    "]\n",
    "\n",
    "class OllamaChatModel:\n",
    "    \"\"\"\n",
    "    Wrapper for the Ollama chat model to mimic the interface expected by create_codeact.\n",
    "    \n",
    "    This class provides a stream method that accepts input messages,\n",
    "    calls the Ollama API, and yields output in the expected format.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def invoke(self, messages: dict, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Synchronously invoke the model by wrapping the streaming interface.\n",
    "        This method calls `stream` and returns the first message object.\n",
    "        \"\"\"\n",
    "        for typ, chunk in self.stream(messages, **kwargs):\n",
    "            if typ == \"messages\":\n",
    "                # Return the first message from the chunk\n",
    "                return chunk[0]\n",
    "        # Optionally, raise an error if no message was returned.\n",
    "        raise ValueError(\"No message returned from stream invocation.\")\n",
    "    \n",
    "    def stream(self, inputs: dict, stream_mode=None, config=None):\n",
    "        \"\"\"\n",
    "        Mimic streaming behavior for the chat agent.\n",
    "        \n",
    "        Parameters:\n",
    "            inputs: Either a dictionary with a 'messages' key or directly a list of messages.\n",
    "            stream_mode: (Optional) List of modes to stream.\n",
    "            config: (Optional) Additional configuration.\n",
    "                \n",
    "        Yields:\n",
    "            A tuple (typ, chunk) where:\n",
    "            - typ \"messages\" contains a list with a single message object that has a 'content' attribute.\n",
    "            - typ \"values\" yields a dummy dictionary (if needed by the agent).\n",
    "        \"\"\"\n",
    "        # Support both dict input and list input\n",
    "        if isinstance(inputs, dict):\n",
    "            messages = inputs.get(\"messages\", [])\n",
    "        elif isinstance(inputs, list):\n",
    "            messages = inputs\n",
    "        else:\n",
    "            raise TypeError(\"Invalid input type for stream; expected dict or list.\")\n",
    "\n",
    "        # Call the Ollama API with the provided messages\n",
    "        response = ollama.chat(model=self.model_name, messages=messages)\n",
    "        # Create a simple message object with a 'content' attribute\n",
    "        message_obj = type(\"Message\", (), {})()\n",
    "        message_obj.content = response['message']['content']\n",
    "        # Yield the response similar to the expected interface\n",
    "        yield (\"messages\", [message_obj])\n",
    "        # Optionally yield additional values if required by downstream processes\n",
    "        yield (\"values\", {\"dummy\": \"value\"})\n",
    "\n",
    "\n",
    "# New agent initialization function using the Ollama model\n",
    "def initialize_agent_with_ollama():\n",
    "    \"\"\"\n",
    "    Initialize the code_act agent using the OllamaChatModel.\n",
    "    \n",
    "    This function replaces the standard init_chat_model call with our Ollama-based implementation.\n",
    "    It integrates the new chat model into create_codeact and compiles the agent.\n",
    "    \"\"\"\n",
    "    # Instantiate the Ollama-based chat model with the desired model name\n",
    "    model = OllamaChatModel(\"deepseek-r1:latest\")\n",
    "    # Create code_act with the new model, our tools list, and the eval function\n",
    "    code_act = create_codeact(model, tools, eval)\n",
    "    # Compile the agent using MemorySaver as the checkpoint\n",
    "    agent = code_act.compile(checkpointer=MemorySaver())\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the agent using the Ollama model wrapper\n",
    "    agent = initialize_agent_with_ollama()\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"A batter hits a baseball at 45.847 m/s at an angle of 23.474° above the horizontal. \"\n",
    "                \"The outfielder, who starts facing the batter, picks up the baseball as it lands, then throws it back \"\n",
    "                \"towards the batter at 24.12 m/s at an angle of 39.12 degrees. How far is the baseball from where the \"\n",
    "                \"batter originally hit it? Assume zero air resistance.\"\n",
    "            ),\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Stream the agent's response and print the output\n",
    "    for typ, chunk in agent.stream(\n",
    "        {\"messages\": messages},\n",
    "        stream_mode=[\"values\", \"messages\"],\n",
    "        config={\"configurable\": {\"thread_id\": 1}},\n",
    "    ):\n",
    "        if typ == \"messages\":\n",
    "            print(chunk[0].content, end=\"\")\n",
    "        elif typ == \"values\":\n",
    "            print(\"\\n\\n---answer---\\n\\n\", chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
