{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data into Neptune, optimize and monitor the data loading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import requests\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import time\n",
    "import json\n",
    "\n",
    "# ====================\n",
    "# Configuration Parameters\n",
    "# ====================\n",
    "# S3 details\n",
    "S3_BUCKET = \"your-s3-bucket-name\"             # e.g., \"my-neptune-bulk-load-bucket\"\n",
    "S3_KEY = \"neptune/neptune_bulk_data.json\"      # S3 key (path) where the file will be stored\n",
    "FILE_PATH = \"neptune_bulk_data.json\"           # Local file generated from our metadata\n",
    "\n",
    "# Neptune bulk loader settings\n",
    "NEPTUNE_ENDPOINT = \"your-neptune-endpoint:8182\" # e.g., \"neptune-cluster.cluster-abcdefg.us-east-1.neptune.amazonaws.com:8182\"\n",
    "IAM_ROLE_ARN = \"arn:aws:iam::your-account-id:role/your-neptune-s3-role\"  # IAM role for Neptune to access S3\n",
    "AWS_REGION = \"your-region\"                       # e.g., \"us-east-1\"\n",
    "\n",
    "# ====================\n",
    "# Step 1: Metadata Generation and JSON Creation\n",
    "# ====================\n",
    "\n",
    "# Fake metadata from Oracle\n",
    "oracle_metadata = {\n",
    "    \"database\": \"OracleDB\",\n",
    "    \"schema\": \"HR\",\n",
    "    \"tables\": [\n",
    "        {\n",
    "            \"table_name\": \"EMPLOYEES\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"EMP_ID\", \"data_type\": \"NUMBER\", \"indexed\": True},\n",
    "                {\"name\": \"FIRST_NAME\", \"data_type\": \"VARCHAR2\", \"indexed\": False},\n",
    "                {\"name\": \"LAST_NAME\", \"data_type\": \"VARCHAR2\", \"indexed\": False},\n",
    "                {\"name\": \"HIRE_DATE\", \"data_type\": \"DATE\", \"indexed\": False}\n",
    "            ],\n",
    "            \"partitioning\": \"RANGE (HIRE_DATE)\",\n",
    "            \"indexes\": [\n",
    "                {\"name\": \"IDX_EMP_ID\", \"columns\": [\"EMP_ID\"]}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Fake metadata from Snowflake\n",
    "snowflake_metadata = {\n",
    "    \"database\": \"SnowflakeDB\",\n",
    "    \"schema\": \"PUBLIC\",\n",
    "    \"tables\": [\n",
    "        {\n",
    "            \"table_name\": \"CUSTOMERS\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"CUSTOMER_ID\", \"data_type\": \"NUMBER\", \"indexed\": True},\n",
    "                {\"name\": \"NAME\", \"data_type\": \"STRING\", \"indexed\": False},\n",
    "                {\"name\": \"EMAIL\", \"data_type\": \"STRING\", \"indexed\": True}\n",
    "            ],\n",
    "            \"partitioning\": \"CLUSTER BY (CUSTOMER_ID)\",\n",
    "            \"indexes\": [\n",
    "                {\"name\": \"IDX_CUSTOMER_ID\", \"columns\": [\"CUSTOMER_ID\"]},\n",
    "                {\"name\": \"IDX_EMAIL\", \"columns\": [\"EMAIL\"]}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Fake metadata from DuckDB\n",
    "duckdb_metadata = {\n",
    "    \"database\": \"DuckDB\",\n",
    "    \"schema\": \"main\",\n",
    "    \"tables\": [\n",
    "        {\n",
    "            \"table_name\": \"SALES\",\n",
    "            \"columns\": [\n",
    "                {\"name\": \"SALE_ID\", \"data_type\": \"INTEGER\", \"indexed\": True},\n",
    "                {\"name\": \"AMOUNT\", \"data_type\": \"DOUBLE\", \"indexed\": False},\n",
    "                {\"name\": \"SALE_DATE\", \"data_type\": \"DATE\", \"indexed\": False}\n",
    "            ],\n",
    "            \"partitioning\": \"NONE\",\n",
    "            \"indexes\": [\n",
    "                {\"name\": \"IDX_SALE_ID\", \"columns\": [\"SALE_ID\"]}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "def generate_neptune_bulk_data(metadatas):\n",
    "    \"\"\"\n",
    "    Converts metadata from multiple database sources into Neptune bulk loader JSON format.\n",
    "    Creates vertices and edges for Database, Schema, Table, Column, and Index entities.\n",
    "    \"\"\"\n",
    "    bulk_data = []\n",
    "    timestamp = datetime.utcnow().isoformat()\n",
    "\n",
    "    # Create vertices and edges for each data source.\n",
    "    for source in metadatas:\n",
    "        db_name = source[\"database\"]\n",
    "        schema_name = source[\"schema\"]\n",
    "\n",
    "        # Vertex: Database\n",
    "        bulk_data.append({\n",
    "            \"id\": f\"db_{db_name}\",\n",
    "            \"label\": \"Database\",\n",
    "            \"properties\": {\n",
    "                \"name\": db_name,\n",
    "                \"source\": source.get(\"source\", \"unknown\"),\n",
    "                \"created\": timestamp\n",
    "            }\n",
    "        })\n",
    "\n",
    "        # Vertex: Schema\n",
    "        bulk_data.append({\n",
    "            \"id\": f\"schema_{db_name}_{schema_name}\",\n",
    "            \"label\": \"Schema\",\n",
    "            \"properties\": {\n",
    "                \"name\": schema_name,\n",
    "                \"database\": db_name,\n",
    "                \"created\": timestamp\n",
    "            }\n",
    "        })\n",
    "\n",
    "        # Process each table in the source schema.\n",
    "        for table in source[\"tables\"]:\n",
    "            table_name = table[\"table_name\"]\n",
    "            table_id = f\"table_{db_name}_{schema_name}_{table_name}\"\n",
    "\n",
    "            # Vertex: Table\n",
    "            bulk_data.append({\n",
    "                \"id\": table_id,\n",
    "                \"label\": \"Table\",\n",
    "                \"properties\": {\n",
    "                    \"name\": table_name,\n",
    "                    \"schema\": schema_name,\n",
    "                    \"database\": db_name,\n",
    "                    \"partitioning\": table[\"partitioning\"],\n",
    "                    \"created\": timestamp\n",
    "                }\n",
    "            })\n",
    "\n",
    "            # Edge: Schema contains Table\n",
    "            bulk_data.append({\n",
    "                \"id\": f\"edge_{table_id}_in_schema\",\n",
    "                \"label\": \"contains\",\n",
    "                \"from\": f\"schema_{db_name}_{schema_name}\",\n",
    "                \"to\": table_id,\n",
    "                \"properties\": {\n",
    "                    \"created\": timestamp\n",
    "                }\n",
    "            })\n",
    "\n",
    "            # Process each column in the table.\n",
    "            for col in table[\"columns\"]:\n",
    "                col_name = col[\"name\"]\n",
    "                col_id = f\"column_{db_name}_{schema_name}_{table_name}_{col_name}\"\n",
    "                bulk_data.append({\n",
    "                    \"id\": col_id,\n",
    "                    \"label\": \"Column\",\n",
    "                    \"properties\": {\n",
    "                        \"name\": col_name,\n",
    "                        \"data_type\": col[\"data_type\"],\n",
    "                        \"indexed\": str(col[\"indexed\"]),\n",
    "                        \"table\": table_name,\n",
    "                        \"created\": timestamp\n",
    "                    }\n",
    "                })\n",
    "\n",
    "                # Edge: Table has Column\n",
    "                bulk_data.append({\n",
    "                    \"id\": f\"edge_{table_id}_{col_id}_has_column\",\n",
    "                    \"label\": \"has_column\",\n",
    "                    \"from\": table_id,\n",
    "                    \"to\": col_id,\n",
    "                    \"properties\": {\n",
    "                        \"created\": timestamp\n",
    "                    }\n",
    "                })\n",
    "\n",
    "            # Process indexes on the table.\n",
    "            for idx in table[\"indexes\"]:\n",
    "                idx_name = idx[\"name\"]\n",
    "                idx_id = f\"index_{db_name}_{schema_name}_{table_name}_{idx_name}\"\n",
    "                bulk_data.append({\n",
    "                    \"id\": idx_id,\n",
    "                    \"label\": \"Index\",\n",
    "                    \"properties\": {\n",
    "                        \"name\": idx_name,\n",
    "                        \"table\": table_name,\n",
    "                        \"columns\": json.dumps(idx[\"columns\"]),\n",
    "                        \"created\": timestamp\n",
    "                    }\n",
    "                })\n",
    "\n",
    "                # Edge: Table has Index\n",
    "                bulk_data.append({\n",
    "                    \"id\": f\"edge_{table_id}_{idx_id}_has_index\",\n",
    "                    \"label\": \"has_index\",\n",
    "                    \"from\": table_id,\n",
    "                    \"to\": idx_id,\n",
    "                    \"properties\": {\n",
    "                        \"created\": timestamp\n",
    "                    }\n",
    "                })\n",
    "    return bulk_data\n",
    "\n",
    "def save_bulk_data_to_file(bulk_data, file_path):\n",
    "    \"\"\"\n",
    "    Saves the Neptune bulk loader data as JSON to a local file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(bulk_data, f, indent=2)\n",
    "    print(f\"Neptune bulk loader data generated and saved to '{file_path}'.\")\n",
    "\n",
    "# ====================\n",
    "# Step 2: S3 Upload Functionality\n",
    "# ====================\n",
    "def upload_to_s3(file_path, bucket, key):\n",
    "    \"\"\"\n",
    "    Uploads a local file to the specified S3 bucket and key.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    try:\n",
    "        s3_client.upload_file(file_path, bucket, key)\n",
    "        print(f\"File successfully uploaded to: s3://{bucket}/{key}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error uploading file to S3:\", e)\n",
    "        raise e  # Reraise exception for further handling\n",
    "\n",
    "# ====================\n",
    "# Step 3: Trigger Neptune Bulk Loader\n",
    "# ====================\n",
    "def trigger_neptune_loader(neptune_endpoint, bucket, key, iam_role_arn, aws_region):\n",
    "    \"\"\"\n",
    "    Triggers the Neptune bulk loader by posting a payload to its HTTP endpoint.\n",
    "    \"\"\"\n",
    "    loader_url = f\"https://{neptune_endpoint}/loader\"\n",
    "    loader_payload = {\n",
    "        \"source\": f\"s3://{bucket}/{key}\",\n",
    "        \"format\": \"json\",           # Format is \"json\" for our bulk loader JSON file\n",
    "        \"iamRoleArn\": iam_role_arn,\n",
    "        \"region\": aws_region,\n",
    "        \"failOnError\": \"FALSE\",\n",
    "        \"parallelism\": \"LOW\"        # Adjust as necessary: LOW, MEDIUM, or HIGH\n",
    "    }\n",
    "    print(\"Initiating Neptune bulk load...\")\n",
    "    try:\n",
    "        # In testing, you may disable SSL verification with verify=False.\n",
    "        response = requests.post(loader_url, json=loader_payload, verify=False)\n",
    "        response.raise_for_status()  # Raise exception on HTTP error status\n",
    "        print(\"Neptune bulk load initiated successfully. Response:\")\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "    except Exception as e:\n",
    "        print(\"Error initiating Neptune bulk load:\", e)\n",
    "        raise e\n",
    "\n",
    "\n",
    "# ====================\n",
    "# New Step 4: LLM Integration Using Neptune as Backend\n",
    "# ====================\n",
    "\n",
    "def query_neptune_data(sparql_query):\n",
    "    \"\"\"\n",
    "    Queries the Neptune SPARQL endpoint with the provided query.\n",
    "    Returns the results in JSON format.\n",
    "    \"\"\"\n",
    "    query_url = f\"https://{NEPTUNE_ENDPOINT}/sparql\"\n",
    "    headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "    params = {\"query\": sparql_query}\n",
    "    try:\n",
    "        response = requests.get(query_url, headers=headers, params=params, verify=False)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(\"Error during Neptune SPARQL query:\", e)\n",
    "        raise e\n",
    "\n",
    "def llm_suggest_optimizations(metadata):\n",
    "    \"\"\"\n",
    "    Uses Ollama LLM via LangChain to analyze metadata and suggest database optimizations.\n",
    "    \n",
    "    Args:\n",
    "        metadata (dict): Database metadata from multiple sources.\n",
    "        \n",
    "    Returns:\n",
    "        str: LLM-generated suggestions.\n",
    "    \"\"\"\n",
    "    # Initialize local LLM model\n",
    "    llm = Ollama(model=\"llama3:latest\")\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"metadata\"],\n",
    "        template=(\n",
    "            \"Given the database metadata:\\n\"\n",
    "            \"{metadata}\\n\\n\"\n",
    "            \"Suggest possible indexing or query optimizations to improve database performance.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Create LangChain LLM chain\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    # Track latency\n",
    "    start_time = time.perf_counter()\n",
    "    suggestion = chain.run(metadata=json.dumps(metadata, indent=2))\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "    print(f\"LLM Suggestion Latency: {elapsed_time:.3f}s\")\n",
    "    \n",
    "    return suggestion\n",
    "\n",
    "\n",
    "\n",
    "# ====================\n",
    "# Main Routine to Execute All Steps\n",
    "# ====================\n",
    "def main():\n",
    "    # Combine metadata from all three sources\n",
    "    all_metadata = [oracle_metadata, snowflake_metadata, duckdb_metadata]\n",
    "    \n",
    "    # Generate Neptune bulk loader data\n",
    "    neptune_bulk_data = generate_neptune_bulk_data(all_metadata)\n",
    "    \n",
    "    # Save bulk data to local file\n",
    "    save_bulk_data_to_file(neptune_bulk_data, FILE_PATH)\n",
    "    \n",
    "    # Upload file to S3\n",
    "    upload_to_s3(FILE_PATH, S3_BUCKET, S3_KEY)\n",
    "    \n",
    "    # Trigger Neptune bulk loader using the uploaded file\n",
    "    trigger_neptune_loader(NEPTUNE_ENDPOINT, S3_BUCKET, S3_KEY, IAM_ROLE_ARN, AWS_REGION)\n",
    "\n",
    "\n",
    "    # Invoke LLM suggestions after metadata generation\n",
    "    optimizations = llm_suggest_optimizations(all_metadata)\n",
    "    print(\"\\nLLM Optimizations:\\n\", optimizations)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load llm back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "            \n",
    "def generate_python_function(description): \n",
    "    response = ollama.chat(model='deepseek-r1:latest', messages=[\n",
    "        {'role': 'user', 'content': f'Generate a Python function for: {description}'}    \n",
    "    ])   \n",
    "    return response['message']['content']\n",
    "            \n",
    "# Generate a Python function for generating the Fibonacci sequence\n",
    "code = generate_python_function('fibonacci sequence generator')\n",
    "            \n",
    "\n",
    "print(code)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input LLM in UI for use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run this:\n",
    "docker build -t chatbot-ollama .\n",
    "docker run -p 3000:3000 chatbot-ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input LLM into sandbox for codeact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import contextlib\n",
    "import io\n",
    "from typing import Any\n",
    "\n",
    "# ---------------------------\n",
    "# New Function: sandbox_run\n",
    "# ---------------------------\n",
    "def sandbox_run(code: str, env: dict[str, Any]) -> tuple[str, dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Execute code in a restricted environment and capture its stdout output.\n",
    "    \n",
    "    Args:\n",
    "        code (str): The Python code to execute.\n",
    "        env (dict): A dictionary representing the environment (functions, variables) available to the code.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing the output string and any new variables created during execution.\n",
    "    \"\"\"\n",
    "    # Create a copy of the environment to avoid side effects.\n",
    "    _locals = env.copy()\n",
    "    original_keys = set(_locals.keys())\n",
    "    try:\n",
    "        # Capture printed output.\n",
    "        f = io.StringIO()\n",
    "        with contextlib.redirect_stdout(f):\n",
    "            exec(code, {\"__builtins__\": builtins.__dict__}, _locals)\n",
    "        result_output = f.getvalue().strip()\n",
    "        if result_output == \"\":\n",
    "            result_output = \"<no output>\"\n",
    "    except Exception as e:\n",
    "        result_output = f\"Error during execution: {e}\"\n",
    "    # Capture any new variables created.\n",
    "    new_vars = {k: _locals[k] for k in _locals.keys() - original_keys}\n",
    "    return result_output, new_vars\n",
    "\n",
    "# ---------------------------\n",
    "# New Function: format_graph_result\n",
    "# ---------------------------\n",
    "def format_graph_result(result: Any) -> str:\n",
    "    \"\"\"\n",
    "    Convert a Neptune query result into a user-friendly graph format (e.g., Cypher-like nodes and edges).\n",
    "    \n",
    "    For demonstration purposes, this function simply converts the result to a string.\n",
    "    You can extend it to parse the result and produce formatted node-edge patterns.\n",
    "    \n",
    "    Args:\n",
    "        result (Any): The raw result from a Neptune query.\n",
    "    \n",
    "    Returns:\n",
    "        str: A user-friendly representation of the graph query result.\n",
    "    \"\"\"\n",
    "    # Placeholder: Convert result to string; expand with custom formatting as needed.\n",
    "    return str(result)\n",
    "\n",
    "# ---------------------------\n",
    "# New Function: create_neptune_optimizer_agent\n",
    "# ---------------------------\n",
    "def create_neptune_optimizer_agent():\n",
    "    \"\"\"\n",
    "    Creates and compiles a CodeAct agent that integrates with Amazon Neptune.\n",
    "    \n",
    "    The agent uses the following tools:\n",
    "      - query_neptune_data: To run SPARQL/Gremlin queries.\n",
    "      - llm_suggest_optimizations: To provide optimization suggestions.\n",
    "      - generate_neptune_bulk_data: (Optional) To generate bulk data.\n",
    "      - format_graph_result: To format raw query results in a user-friendly, Cypher-like format.\n",
    "      \n",
    "    The agent is built using the LangGraph CodeAct framework with an in-memory checkpointer for session state.\n",
    "    \n",
    "    Returns:\n",
    "        agent: The compiled CodeAct agent.\n",
    "    \"\"\"\n",
    "    # Import necessary modules from LangGraph (ensure these libraries are installed)\n",
    "    from langgraph_codeact import create_codeact\n",
    "    from langgraph.checkpoint.memory import MemorySaver\n",
    "    # Import the chat model. We assume you have access to ChatOllama; otherwise, you may use your existing Ollama import.\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "    # Initialize the chat model for Llama3 (adjust the model tag as needed)\n",
    "    chat_model = ChatOllama(model=\"llama3:70b\")  # This uses a 70B instruct variant.\n",
    "\n",
    "    # List of available tools for the agent.\n",
    "    # (Make sure these functions are defined in your codebase.)\n",
    "    tools = [\n",
    "        query_neptune_data,         # Executes Neptune queries.\n",
    "        llm_suggest_optimizations,  # Provides optimization suggestions.\n",
    "        generate_neptune_bulk_data,   # Generates bulk load data (if needed by the agent).\n",
    "        format_graph_result         # Formats query results.\n",
    "    ]\n",
    "\n",
    "    # Create the CodeAct graph agent with our sandbox_run for code execution.\n",
    "    code_act_graph = create_codeact(chat_model, tools, sandbox_run, prompt_template=None)\n",
    "    # Compile the agent using an in-memory checkpointer for state persistence.\n",
    "    agent = code_act_graph.compile(checkpointer=MemorySaver())\n",
    "    return agent\n",
    "\n",
    "# ---------------------------\n",
    "# (Optional) Example Function: neptune_agent_interface\n",
    "# ---------------------------\n",
    "def neptune_agent_interface():\n",
    "    \"\"\"\n",
    "    An example interface to interact with the Neptune Optimizer Agent.\n",
    "    \n",
    "    This function simulates a conversation loop where a user can send queries\n",
    "    or optimization requests. The agent preserves context between turns.\n",
    "    \"\"\"\n",
    "    # Create the agent.\n",
    "    agent = create_neptune_optimizer_agent()\n",
    "    \n",
    "    # Example conversation configuration (using thread_id to preserve session state).\n",
    "    thread_config = {\"configurable\": {\"thread_id\": 1}}\n",
    "    \n",
    "    # Example 1: Data Query Request\n",
    "    user_query = \"Find all authors who collaborated with 'Alice' within the last year.\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_query}]\n",
    "    result_state = agent.invoke({\"messages\": messages}, config=thread_config)\n",
    "    print(\"Agent Answer (Data Query):\", result_state.get(\"output\"))\n",
    "    \n",
    "    # Example 2: Optimization Request\n",
    "    user_query2 = \"How can I optimize queries that look up collaborations on the Sales table?\"\n",
    "    messages = [{\"role\": \"user\", \"content\": user_query2}]\n",
    "    result_state = agent.invoke({\"messages\": messages}, config=thread_config)\n",
    "    print(\"Agent Answer (Optimization):\", result_state.get(\"output\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
