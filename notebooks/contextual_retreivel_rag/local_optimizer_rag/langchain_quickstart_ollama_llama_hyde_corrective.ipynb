{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG, RAG with Memory, Adaptive RAG, Corrective RAG, self-RAG, Agentive RAG... are you lost? Let me help you with this guide.\n",
    "\n",
    "1/ Simple RAG\n",
    "Retrieves relevant documents based on the query and uses them to generate an answer.\n",
    "\n",
    "2/ Simple RAG with Memory\n",
    "Extends Simple RAG by maintaining context from previous interactions.\n",
    "\n",
    "3/ Branched RAG\n",
    "Performs multiple retrieval steps, refining the search based on intermediate results.\n",
    "\n",
    "4/ HyDE (Hypothetical Document Embedding)\n",
    "Generates a hypothetical ideal document before retrieval to improve search relevance.\n",
    "\n",
    "5/ Adaptive RAG\n",
    "Dynamically adjusts retrieval and generation strategies based on the query type or difficulty.\n",
    "\n",
    "6/ Corrective RAG (CRAG)\n",
    "Iteratively refines generated responses by fact-checking against retrieved information.\n",
    "\n",
    "7/ Self-RAG\n",
    "The model critiques and improves its own responses using self-reflection and retrieval.\n",
    "\n",
    "8/ Agentic RAG\n",
    "Combines RAG with agentic behavior, allowing for more complex, multi-step problem-solving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.1/docs/get_started/quickstart/\n",
    "\n",
    "langchain quick start ^\n",
    "\n",
    "\n",
    "https://python.langchain.com/docs/integrations/providers/ollama/\n",
    "\n",
    "Ollama integrations ^\n",
    "\n",
    "Tool calling:\n",
    "https://ollama.com/blog/tool-support\n",
    "https://python.langchain.com/docs/how_to/tool_calling/\n",
    "\n",
    "\n",
    "- Easy example:\n",
    "https://github.com/Shubhamsaboo/awesome-llm-apps/blob/main/llama3.1_local_rag/llama3.1_local_rag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.0+cu124\n",
      "CUDA is available! GPU is ready to be used.\n",
      "Number of GPUs available: 1\n",
      "Current GPU: NVIDIA GeForce RTX 4090\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "  - Total Memory: 25.756696576 GB\n",
      "  - Compute Capability: (8, 9)\n",
      "Tensor on GPU: tensor([[0.3241, 0.3342, 0.9298],\n",
      "        [0.4177, 0.4081, 0.3960],\n",
      "        [0.2993, 0.5588, 0.3605]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! GPU is ready to be used.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. GPU is not set up correctly.\")\n",
    "\n",
    "# Print additional GPU details\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  - Total Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9} GB\")\n",
    "        print(f\"  - Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Create a random tensor and move it to the GPU\n",
    "    tensor = torch.rand(3, 3).cuda()\n",
    "    print(\"Tensor on GPU:\", tensor)\n",
    "else:\n",
    "    print(\"GPU is not available, cannot move tensor to GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/custom_ollama_docker/notebooks/contextual_retreivel_rag/local_optimizer_rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Print the current working directory (optional for debugging)\n",
    "print(os.getcwd())\n",
    "\n",
    "# Set the path to your .env file relative to the current working directory\n",
    "dotenv_path = os.path.join(os.getcwd(), '../../.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "# Set up API keys\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/modules/data_crawling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/modules/data_crawling.py\n",
    "\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "\n",
    "def crawl_and_ingest(directory_path, file_types=None, debug=False):\n",
    "    \"\"\"\n",
    "    Crawls the specified directory for files with given extensions,\n",
    "    processes the contents, and returns documents ready for RAG ingestion.\n",
    "\n",
    "    Parameters:\n",
    "    - directory_path: Path to the directory to crawl.\n",
    "    - file_types: List of file extensions to include (e.g., [\".py\", \".md\", \".csv\"]). \n",
    "                  Default includes various code and data formats.\n",
    "    - debug: Boolean flag to print debug information.\n",
    "    \"\"\"\n",
    "    # Expanded default file types to include more programming languages and data formats\n",
    "    if file_types is None:\n",
    "        file_types = [\".py\", \".ipynb\", \".txt\", \".md\", \".csv\", \".js\", \".html\", \n",
    "                      \".css\", \".json\", \".yaml\", \".yml\", \".xml\", \".r\", \".cpp\", \n",
    "                      \".java\", \".scala\", \".sql\"]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Starting to load files from directory: {directory_path} with file types: {file_types}\")\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # Traverse the directory for specified file types\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            if any(file.endswith(ext) for ext in file_types):\n",
    "                if debug:\n",
    "                    print(f\"Found file: {file_path}\")\n",
    "\n",
    "                if file.endswith(\".csv\"):  # CSV files\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, on_bad_lines = 'skip')\n",
    "                        if df.empty:\n",
    "                            if debug:\n",
    "                                print(f\"CSV file is empty: {file_path}\")\n",
    "                            continue\n",
    "                        content = df.to_string()  # Convert to string for ingestion\n",
    "                        docs.append(Document(page_content=content, metadata={\"file_name\": file_path}))\n",
    "                    except pd.errors.EmptyDataError:\n",
    "                        if debug:\n",
    "                            print(f\"Empty CSV file skipped: {file_path}\")\n",
    "\n",
    "                elif file.endswith(\".ipynb\"):  # Jupyter Notebooks\n",
    "                    try:\n",
    "                        loader = NotebookLoader(file_path, include_outputs=False, max_output_length=0)\n",
    "                        notebook_docs = loader.load()\n",
    "                        docs.extend(notebook_docs)\n",
    "                    except Exception as e:\n",
    "                        if debug:\n",
    "                            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "                else:  # Text and other code files\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    docs.append(Document(page_content=content, metadata={\"file_name\": file_path}))\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Total documents loaded: {len(docs)}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "def process_chunk(chunk, debug=False):\n",
    "    \"\"\"\n",
    "    Generates and quality checks propositions for a given chunk.\n",
    "    \"\"\"\n",
    "    propositions = generate_propositions(chunk.page_content, debug)\n",
    "    high_quality_propositions = quality_check_propositions(propositions, debug)\n",
    "    return [Document(page_content=prop) for prop in high_quality_propositions]\n",
    "\n",
    "def generate_propositions(text, debug=False):\n",
    "    \"\"\"\n",
    "    Generates propositions from the given text using an LLM.\n",
    "    \"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    max_length = 2000\n",
    "    text = text[:max_length] if len(text) > max_length else text\n",
    "\n",
    "    proposition_prompt = (\n",
    "        f\"Break down the following text into concise, complete, and meaningful factual statements:\\n\\n{text}\\n\\n\"\n",
    "        \"Provide each proposition as a separate statement.\"\n",
    "    )\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": proposition_prompt}]).content\n",
    "\n",
    "    propositions = [prop.strip() for prop in response.split('\\n') if prop.strip()]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Generated propositions: {propositions[:5]}...\")  # Print first 5 propositions for brevity\n",
    "\n",
    "    return propositions\n",
    "\n",
    "def quality_check_propositions(propositions, debug=False):\n",
    "    \"\"\"\n",
    "    Checks the quality of the propositions for accuracy, clarity, completeness, and conciseness.\n",
    "    \"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    high_quality_propositions = []\n",
    "\n",
    "    batch_size = 5\n",
    "    for i in range(0, len(propositions), batch_size):\n",
    "        batch = propositions[i:i + batch_size]\n",
    "        quality_prompt = (\n",
    "            f\"Evaluate the following propositions for accuracy, clarity, completeness, and conciseness. \"\n",
    "            f\"Score each aspect from 1 to 10 and provide an overall assessment. Reply with 'pass' if the proposition is acceptable:\\n\\n\"\n",
    "            f\"{', '.join(batch)}\"\n",
    "        )\n",
    "        response = llm.invoke([{\"role\": \"user\", \"content\": quality_prompt}]).content\n",
    "\n",
    "        results = response.lower().split('\\n')\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Batch being processed: {batch}\")\n",
    "            print(f\"LLM Response: {response}\")\n",
    "            print(f\"Number of results received: {len(results)}, Number of propositions in batch: {len(batch)}\")\n",
    "\n",
    "        min_length = min(len(results), len(batch))\n",
    "        for j in range(min_length):\n",
    "            if 'pass' in results[j]:\n",
    "                high_quality_propositions.append(batch[j])\n",
    "\n",
    "    return high_quality_propositions\n",
    "\n",
    "\n",
    "def main(debug=False):\n",
    "    # Specify the local repo path and file types to include\n",
    "    directory_path = \"../../../\"\n",
    "    # Expanded file types to include various code and documentation formats\n",
    "    file_types = [\".py\", \".md\", \".csv\", \".ipynb\", \".html\", \".json\", \".yaml\", \".r\", \".cpp\", \".java\", \".scala\", \".sql\"]\n",
    "    documents = crawl_and_ingest(directory_path, file_types, debug)\n",
    "    if debug:\n",
    "        print(f\"Total documents processed for ingestion: {len(documents)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/modules/git_data_crawling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/modules/git_data_crawling.py\n",
    "\n",
    "import requests\n",
    "from langchain.schema import Document\n",
    "\n",
    "def extract_repo_info(repo_url):\n",
    "    \"\"\"\n",
    "    Extracts the username and repository name from a GitHub URL.\n",
    "    \"\"\"\n",
    "    parts = repo_url.rstrip('/').split('/')\n",
    "    if len(parts) < 5:\n",
    "        raise ValueError(\"Invalid GitHub repository URL. Must be in the format: https://github.com/username/repo\")\n",
    "    return parts[-2], parts[-1]\n",
    "\n",
    "def fetch_repo_tree(username, repo_name, debug=False):\n",
    "    \"\"\"\n",
    "    Fetches the main branch file tree from a GitHub repository.\n",
    "    \"\"\"\n",
    "    api_url = f\"https://api.github.com/repos/{username}/{repo_name}/git/trees/main?recursive=1\"\n",
    "    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
    "    \n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    tree = response.json().get(\"tree\", [])\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Fetched {len(tree)} items from GitHub API for {repo_name}\")\n",
    "    \n",
    "    return tree\n",
    "\n",
    "def load_file_content(username, repo_name, file_info, debug=False):\n",
    "    \"\"\"\n",
    "    Loads the content of a single file from a GitHub repository.\n",
    "    \"\"\"\n",
    "    file_url = f\"https://raw.githubusercontent.com/{username}/{repo_name}/main/{file_info['path']}\"\n",
    "    file_content = requests.get(file_url).text\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Loaded file: {file_info['path']}\")\n",
    "    \n",
    "    return Document(page_content=file_content, metadata={\"file_name\": file_info[\"path\"]})\n",
    "\n",
    "def load_github_repo(repo_url, file_types=None, debug=False):\n",
    "    \"\"\"\n",
    "    Load the main branch files from a GitHub repository and return them as documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - repo_url: URL of the GitHub repository.\n",
    "    - file_types: List of file extensions to include (e.g., [\".py\", \".md\", \".csv\"]). \n",
    "                  Defaults to a broad selection of code and data formats.\n",
    "    - debug: Boolean flag to print debug information.\n",
    "    \"\"\"\n",
    "    username, repo_name = extract_repo_info(repo_url)\n",
    "    tree = fetch_repo_tree(username, repo_name, debug)\n",
    "    \n",
    "    # Default file types if none are provided\n",
    "    if file_types is None:\n",
    "        file_types = [\n",
    "            \".py\", \".ipynb\", \".txt\", \".md\", \".csv\", \".js\", \".html\", \n",
    "            \".css\", \".json\", \".yaml\", \".yml\", \".xml\", \".r\", \".cpp\", \n",
    "            \".java\", \".scala\", \".sql\"\n",
    "        ]\n",
    "    \n",
    "    documents = []\n",
    "    for file_info in tree:\n",
    "        if file_info[\"type\"] == \"blob\" and any(file_info[\"path\"].endswith(ext) for ext in file_types):\n",
    "            document = load_file_content(username, repo_name, file_info, debug)\n",
    "            documents.append(document)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Total files loaded from GitHub repository '{repo_name}': {len(documents)}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def main():\n",
    "    # Test URL - Replace this with any public GitHub repository URL\n",
    "    test_repo_url = \"https://github.com/ghadfield32/coach_analysis\"  # Example URL\n",
    "    \n",
    "    # Enable debug to view process details\n",
    "    debug = True\n",
    "    \n",
    "    # Specify file types to include, if desired\n",
    "    file_types = [\".py\", \".md\", \".csv\", \".ipynb\", \".html\", \".json\", \".yaml\", \".r\", \".cpp\", \".java\", \".scala\", \".sql\"]\n",
    "    \n",
    "    try:\n",
    "        documents = load_github_repo(test_repo_url, file_types, debug)\n",
    "        print(\"\\nLoaded documents:\")\n",
    "        for doc in documents:\n",
    "            print(f\"File: {doc.metadata['file_name']} - Content preview: {doc.page_content[:100]}...\")  # Show first 100 characters of each document\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/modules/vector_store.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/modules/vector_store.py\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.schema import Document\n",
    "import os \n",
    "\n",
    "def create_vectorstore(documents, persist_directory='../../../data/chroma_dbs', debug=False):\n",
    "    # Ensure the persistence directory exists\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory)\n",
    "        if debug:\n",
    "            print(f\"Created new persistence directory at {persist_directory}\")\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    if debug:\n",
    "        print(f\"Vector store created at {persist_directory} with {len(documents)} documents.\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def create_pre_ingested_vectorstore(site_name, documents):\n",
    "    # Create directory if it doesn't exist\n",
    "    directory = f\"../../data/vectorstores/{site_name.lower()}\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Create the vector store\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    vectorstore = Chroma.from_documents(documents, embedding=embeddings, persist_directory=directory)\n",
    "    print(f\"Vector store for {site_name} created and saved at {directory}\")\n",
    "\n",
    "def main(debug=False):\n",
    "    # Use a list of high-quality Document objects instead of dictionaries\n",
    "    sample_docs = [Document(page_content=\"This is a high-quality sample document for testing.\")]\n",
    "    vectorstore = create_vectorstore(sample_docs, debug=debug)\n",
    "    if debug:\n",
    "        print(\"Vector store successfully created.\")\n",
    "        \n",
    "    # Example usage:\n",
    "    site_name = \"local_repo_files\"\n",
    "    documents = [Document(page_content=\"This is a sample document for NFL data.\")]\n",
    "    create_pre_ingested_vectorstore(site_name, documents)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/modules/contextual_retrieval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/modules/contextual_retrieval.py\n",
    "\n",
    "import copy\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document\n",
    "\n",
    "def create_contextual_nodes(documents, debug=False):\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    contextual_documents = []\n",
    "\n",
    "    for doc in documents:\n",
    "        context_prompt = (\n",
    "            f\"Generate contextual information for better understanding:\\n\\n{doc.page_content}\\n\\n\"\n",
    "            \"Context:\"\n",
    "        )\n",
    "        context = llm.invoke([{\"role\": \"user\", \"content\": context_prompt}]).content\n",
    "        enriched_doc = copy.deepcopy(doc)\n",
    "        enriched_doc.metadata[\"context\"] = context\n",
    "        contextual_documents.append(enriched_doc)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Generated context for document '{doc.metadata.get('file_name', 'unknown')}'\")\n",
    "\n",
    "    return contextual_documents\n",
    "\n",
    "def create_embedding_retriever(documents, persist_directory='../../../data/chroma_dbs', debug=False):\n",
    "    \"\"\"\n",
    "    Creates a Chroma vector store retriever using contextual nodes.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents (List[Document]): List of contextually enriched Document objects.\n",
    "    - persist_directory (str): Directory to persist the Chroma database.\n",
    "    - debug (bool): Flag for printing debug information.\n",
    "    \n",
    "    Returns:\n",
    "    - Chroma: Chroma vector store retriever object.\n",
    "    \"\"\"\n",
    "    # Create embeddings with Ollama\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    \n",
    "    # Create the Chroma vector store\n",
    "    if debug:\n",
    "        print(f\"Creating vector store with {len(documents)} contextually enriched documents...\")\n",
    "        \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Vector store created at {persist_directory}\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "def main(debug=True):\n",
    "    # Sample documents representing local files\n",
    "    sample_docs = [\n",
    "        Document(page_content=\"This document contains information about file structure optimization.\", metadata={\"file_name\": \"file_structure_optimization.txt\"}),\n",
    "        Document(page_content=\"Guide on improving data storage efficiency.\", metadata={\"file_name\": \"data_storage_efficiency.txt\"})\n",
    "    ]\n",
    "    \n",
    "    # Create contextual nodes\n",
    "    contextual_docs = create_contextual_nodes(sample_docs, debug=debug)\n",
    "    \n",
    "    # Create and test the vector store\n",
    "    vectorstore = create_embedding_retriever(contextual_docs, debug=debug)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Successfully created contextual retriever with {len(contextual_docs)} contextually enriched documents.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/modules/hyde_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/modules/hyde_rag.py\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document\n",
    "\n",
    "def contextual_retrieval(question, retriever, debug=False):\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    hypo_prompt = f\"Answer the question with background knowledge:\\n\\n{question}\\n\\nAnswer:\"\n",
    "    hypo_answer = llm.invoke([{\"role\": \"user\", \"content\": hypo_prompt}]).content\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Hypothetical answer generated: {hypo_answer}\")\n",
    "\n",
    "    retrieved_docs = retriever.invoke(hypo_answer)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Number of documents retrieved: {len(retrieved_docs)}\")\n",
    "        \n",
    "    return retrieved_docs\n",
    "\n",
    "def main(debug=False):\n",
    "    question = \"What are the best practices for optimizing local file storage?\"\n",
    "    \n",
    "    # Example documents on file storage and optimization\n",
    "    sample_docs = [\n",
    "        Document(page_content=\"Methods to optimize file storage efficiency.\", metadata={\"file_name\": \"file_optimization_guide.txt\"})\n",
    "    ]\n",
    "    \n",
    "    # Create contextual nodes and retriever\n",
    "    contextual_docs = create_contextual_nodes(sample_docs, debug=debug)\n",
    "    vectorstore = create_embedding_retriever(contextual_docs, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    # Test the contextual retrieval\n",
    "    contextual_retrieval(question, retriever, debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/modules/corrective_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/modules/corrective_rag.py\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document\n",
    "\n",
    "def corrective_rag(retrieved_docs, debug=False):\n",
    "    \"\"\"\n",
    "    Analyze and make recommendations based on retrieved file chunks.\n",
    "    \"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    recommendations = []\n",
    "\n",
    "    for doc in retrieved_docs:\n",
    "        prompt = (\n",
    "            f\"Based on the following content:\\n\\n{doc.page_content}\\n\\n\"\n",
    "            \"Suggest improvements for file structure, storage efficiency, and best practices.\"\n",
    "        )\n",
    "        recommendation = llm.invoke([{\"role\": \"user\", \"content\": prompt}]).content\n",
    "        recommendations.append(Document(page_content=recommendation, metadata=doc.metadata))\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Recommendation for {doc.metadata.get('file_name', 'unknown')}: {recommendation}\")\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "def main(debug=False):\n",
    "    # Sample document on file structure\n",
    "    retrieved_docs = [\n",
    "        Document(page_content=\"This document covers tips on file organization.\", metadata={\"file_name\": \"file_organization_guide.txt\"})\n",
    "    ]\n",
    "    recommendations = corrective_rag(retrieved_docs, debug=debug)\n",
    "    \n",
    "    if debug:\n",
    "        for recommendation in recommendations:\n",
    "            print(f\"Final recommendation for {recommendation.metadata.get('file_name', 'unknown')}: {recommendation.page_content}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/modules/self_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/modules/self_rag.py\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "def self_rag(question, initial_answer, debug=False):\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    if debug:\n",
    "        print(f\"Initial answer before self-refinement: {initial_answer}\")\n",
    "    \n",
    "    max_reflections = 2\n",
    "    for i in range(max_reflections):\n",
    "        reflect_prompt = f\"Answer: {initial_answer}\\n\\nReflect on the answer and identify areas for improvement.\"\n",
    "        reflection = llm.invoke([{\"role\": \"user\", \"content\": reflect_prompt}]).content\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Reflection result for iteration {i+1}: {reflection}\")\n",
    "\n",
    "        if \"no improvements\" in reflection.lower():\n",
    "            if debug:\n",
    "                print(f\"No further improvements suggested after {i+1} iterations.\")\n",
    "            break\n",
    "        else:\n",
    "            improve_prompt = f\"Based on the reflection: {reflection}\\n\\nProvide an improved answer to the question: {question}\"\n",
    "            initial_answer = llm.invoke([{\"role\": \"user\", \"content\": improve_prompt}]).content\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Improved answer after iteration {i+1}: {initial_answer}\")\n",
    "\n",
    "    return initial_answer\n",
    "\n",
    "def main(debug=False):\n",
    "    question = \"What are effective techniques for optimizing local file storage?\"\n",
    "    initial_answer = \"Local file storage optimization requires strategies such as compression and proper file structure.\"\n",
    "    refined_answer = self_rag(question, initial_answer, debug=debug)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Final refined answer: {refined_answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/modules/web_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/modules/web_search.py\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)\n",
    "\n",
    "def tavily_search(question, debug=False):\n",
    "    docs = tavily_retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join(f\"Source {i+1} ({doc.metadata.get('source')}):\\n{doc.page_content}\" for i, doc in enumerate(docs))\n",
    "    if debug:\n",
    "        print(f\"Web search context retrieved: {context[:500]}...\")  # Display first 500 chars\n",
    "    return context\n",
    "\n",
    "def main(debug=False):\n",
    "    question = \"Tell me about file optimization?\"\n",
    "    context = tavily_search(question, debug)\n",
    "    if debug:\n",
    "        print(f\"Retrieved context from Tavily search: {context}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/modules/decision_mechanism.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/modules/decision_mechanism.py\n",
    "from modules.hyde_rag import contextual_retrieval\n",
    "from modules.corrective_rag import corrective_rag\n",
    "from modules.web_search import tavily_search\n",
    "from modules.self_rag import self_rag  # Include the self_rag module for refinement\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document\n",
    "import streamlit as st\n",
    "\n",
    "def evaluate_confidence(answer, debug=False):\n",
    "    \"\"\"Evaluate the confidence of an answer using a language model.\"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    eval_prompt = (\n",
    "        f\"Evaluate the confidence level (on a scale of 1-10) of the following answer being correct, \"\n",
    "        f\"fully supported by reliable sources, and free from contradictions or inaccuracies:\\n\\n{answer}\\n\\n\"\n",
    "        \"Confidence Score:\"\n",
    "    )\n",
    "    confidence_score = llm.invoke([{\"role\": \"user\", \"content\": eval_prompt}]).content\n",
    "    try:\n",
    "        score = int(confidence_score.strip())\n",
    "    except ValueError:\n",
    "        score = 5  # Default to medium confidence if the evaluation fails\n",
    "    if debug:\n",
    "        print(f\"Confidence score evaluated: {score}\")\n",
    "    return score\n",
    "\n",
    "def decide_and_answer(question, retriever, progress_bar=None, progress_status=None, debug=False):\n",
    "    \"\"\"Generate answers using RAG and Tavily, and decide the best answer with self-refinement.\"\"\"\n",
    "    progress_step = 0.25\n",
    "\n",
    "    # Step 1: Use contextual retrieval to get documents and generate an initial RAG-based answer\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 1/4: Running HyDE retrieval...\")\n",
    "    retrieved_docs = contextual_retrieval(question, retriever, debug)\n",
    "    if progress_bar:\n",
    "        progress_bar.progress(progress_step)\n",
    "\n",
    "    # Step 2: Generate a corrective RAG-based answer\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 2/4: Generating a corrective RAG answer...\")\n",
    "    rag_answer = corrective_rag(retrieved_docs, debug)\n",
    "    rag_refined_answer = self_rag(question, rag_answer, debug)  # Refine RAG answer with self-rag\n",
    "    rag_confidence = evaluate_confidence(rag_refined_answer, debug)\n",
    "    progress_step += 0.25\n",
    "    if progress_bar:\n",
    "        progress_bar.progress(progress_step)\n",
    "\n",
    "    # Step 3: Use Tavily search to generate an answer\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 3/4: Running Tavily search for additional context...\")\n",
    "    tavily_context = tavily_search(question, debug)\n",
    "    tavily_prompt = f\"Context: {tavily_context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    tavily_initial_answer = llm.invoke([{\"role\": \"user\", \"content\": tavily_prompt}]).content\n",
    "    tavily_refined_answer = self_rag(question, tavily_initial_answer, debug)  # Refine Tavily answer with self-rag\n",
    "    tavily_confidence = evaluate_confidence(tavily_refined_answer, debug)\n",
    "    progress_step += 0.25\n",
    "    if progress_bar:\n",
    "        progress_bar.progress(progress_step)\n",
    "\n",
    "    # Step 4: Decision mechanism to choose the final answer based on confidence scores\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 4/4: Making the final decision...\")\n",
    "    if rag_confidence > tavily_confidence:\n",
    "        final_answer = rag_refined_answer\n",
    "        source = \"RAG-based response\"\n",
    "    elif tavily_confidence > rag_confidence:\n",
    "        final_answer = tavily_refined_answer\n",
    "        source = \"Tavily-based response\"\n",
    "    else:\n",
    "        # Combine answers if confidence scores are similar\n",
    "        combined_prompt = (\n",
    "            f\"Here are two potential answers to the question:\\n\\n\"\n",
    "            f\"Answer 1 (RAG-based):\\n{rag_refined_answer}\\n\\n\"\n",
    "            f\"Answer 2 (Tavily-based):\\n{tavily_refined_answer}\\n\\n\"\n",
    "            f\"Based on these, provide the best possible answer to the question: {question}\"\n",
    "        )\n",
    "        final_answer = llm.invoke([{\"role\": \"user\", \"content\": combined_prompt}]).content\n",
    "        source = \"Combined response\"\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Selected final answer from: {source}\")\n",
    "    return final_answer\n",
    "\n",
    "def main(debug=False):\n",
    "    question = \"What are the best methods to organize a large local file repository for efficiency?\"\n",
    "    sample_docs = [\n",
    "        Document(page_content=\"This is a sample document on file organization best practices.\", metadata={\"file_name\": \"organization_best_practices.txt\"})\n",
    "    ]\n",
    "    vectorstore = create_embedding_retriever(create_contextual_nodes(sample_docs, debug=debug), debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Streamlit progress bar and status\n",
    "    progress_bar = st.progress(0)\n",
    "    progress_status = st.empty()\n",
    "\n",
    "    final_answer = decide_and_answer(question, retriever, progress_bar, progress_status, debug)\n",
    "    st.write(f\"Final answer selected: {final_answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/fact_checker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/fact_checker.py\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document\n",
    "\n",
    "from modules.hyde_rag import contextual_retrieval\n",
    "from modules.web_search import tavily_search  \n",
    "from modules.vector_store import create_vectorstore\n",
    "\n",
    "def final_fact_check(question, answer, retriever, debug=False):\n",
    "    \"\"\"\n",
    "    Perform a final fact-check of the answer based on a combined context from retrieved documents and web search results.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): The question asked by the user.\n",
    "    answer (str): The initial answer generated by the RAG or web search.\n",
    "    retriever: The retriever object created from the vector store.\n",
    "    debug (bool): If True, print debug information.\n",
    "\n",
    "    Returns:\n",
    "    str: The fact-checked and potentially corrected answer.\n",
    "    \"\"\"\n",
    "    # Initialize the LLM for fact-checking\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "    # Retrieve documents using HyDE\n",
    "    retrieved_docs = contextual_retrieval(question, retriever, debug=debug)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs) if retrieved_docs else \"\"\n",
    "\n",
    "    # Retrieve web context using Tavily search\n",
    "    tavily_context = tavily_search(question, debug=debug)\n",
    "\n",
    "    # Combine both contexts\n",
    "    combined_context = context + \"\\n\\n\" + tavily_context\n",
    "\n",
    "    # Debug output for context combination\n",
    "    if debug:\n",
    "        print(f\"Combined context for fact-checking:\\n{combined_context}\")\n",
    "\n",
    "    # Create the fact-checking prompt\n",
    "    fact_check_prompt = (\n",
    "        f\"Context: {combined_context}\\n\\nAnswer: {answer}\\n\\n\"\n",
    "        f\"Verify the accuracy of the answer based on the context. Provide a corrected answer if necessary.\"\n",
    "    )\n",
    "\n",
    "    # Generate the fact-checked answer using the LLM\n",
    "    final_answer = llm.invoke([{\"role\": \"user\", \"content\": fact_check_prompt}]).content\n",
    "\n",
    "    # Debug output for final answer\n",
    "    if debug:\n",
    "        print(f\"Fact-checked answer: {final_answer}\")\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "def main(debug=False):\n",
    "    \"\"\"\n",
    "    Test the final_fact_check function with sample input.\n",
    "    \"\"\"\n",
    "    # Sample question and answer for a repository-focused example\n",
    "    question = \"What are the best practices for organizing a local code repository?\"\n",
    "    initial_answer = \"Organize files by language, with folders for Python, JavaScript, and SQL scripts.\"\n",
    "\n",
    "    # Create a sample retriever for repository organization context\n",
    "    sample_docs = [\n",
    "        Document(page_content=\"Best practices for organizing a code repository include structuring folders by project modules, using clear naming conventions, and maintaining a README for documentation.\", metadata={\"file_name\": \"repo_organization_guide.md\"}),\n",
    "        Document(page_content=\"Consider creating separate folders for data, scripts, and tests. A well-documented repository is easier for collaboration and maintenance.\", metadata={\"file_name\": \"repo_best_practices.md\"})\n",
    "    ]\n",
    "    vectorstore = create_vectorstore(sample_docs, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Run the final_fact_check function\n",
    "    corrected_answer = final_fact_check(question, initial_answer, retriever, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"Corrected answer after final fact-check: {corrected_answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../../src/git_repo_model/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/main.py\n",
    "\n",
    "# Import necessary modules for the main workflow\n",
    "from modules.data_crawling import crawl_and_ingest\n",
    "from modules.vector_store import create_vectorstore\n",
    "from modules.decision_mechanism import decide_and_answer\n",
    "from modules.fact_checker import final_fact_check\n",
    "from modules.hyde_rag import contextual_retrieval  # Use the new contextual retrieval function\n",
    "from modules.corrective_rag import corrective_rag  # Import corrective_rag function for code improvements\n",
    "from modules.git_data_crawling import load_github_repo  # Import the GitHub repo loader\n",
    "from modules.hyde_rag import contextual_retrieval\n",
    "\n",
    "def run_rag_pipeline(data_source, repo_path=None, repo_url=None, question=\"What are the best practices for organizing a code repository?\", debug=False):\n",
    "    \"\"\"\n",
    "    Run the RAG pipeline by loading documents from either a local directory or GitHub repository,\n",
    "    creating a vector store, generating recommendations, performing fact-checking, and presenting the final answer.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_source (str): \"Local\" or \"Git\"\n",
    "    - repo_path (str, optional): Local directory path (required if data_source is \"Local\")\n",
    "    - repo_url (str, optional): GitHub repository URL (required if data_source is \"Git\")\n",
    "    - question (str): The question for which recommendations are generated\n",
    "    - debug (bool): Enable debug outputs for step-by-step tracing\n",
    "    \"\"\"\n",
    "    # Define the file types for filtering\n",
    "    file_types = [\".py\", \".md\", \".csv\", \".ipynb\", \".html\", \".json\", \".yaml\", \".r\", \".cpp\", \".java\", \".scala\", \".sql\"]\n",
    "\n",
    "    # Step 1: Load documents based on the selected data source\n",
    "    if data_source == \"Local\":\n",
    "        if not repo_path:\n",
    "            raise ValueError(\"Local directory path must be provided for Local source.\")\n",
    "        documents = crawl_and_ingest(repo_path, file_types, debug)\n",
    "    elif data_source == \"Git\":\n",
    "        if not repo_url:\n",
    "            raise ValueError(\"GitHub repository URL must be provided for Git source.\")\n",
    "        documents = load_github_repo(repo_url, file_types, debug)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid data source selected. Choose either 'Local' or 'Git'.\")\n",
    "    \n",
    "    # Display loaded documents for debugging\n",
    "    if debug:\n",
    "        print(f\"Total documents loaded: {len(documents)}\")\n",
    "    \n",
    "    # Step 2: Create a vector store and a retriever\n",
    "    vectorstore = create_vectorstore(documents, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    # Step 3: Retrieve relevant documents using contextual retrieval\n",
    "    retrieved_docs = contextual_retrieval(question, retriever, debug)\n",
    "    \n",
    "    # Step 4: Generate recommendations using corrective RAG\n",
    "    recommendations = corrective_rag(retrieved_docs, debug=debug)\n",
    "    \n",
    "    # Display initial recommendations\n",
    "    if debug:\n",
    "        for rec in recommendations:\n",
    "            print(f\"Initial recommendation for {rec.metadata.get('file_name', 'unknown')}:\\n{rec.page_content}\")\n",
    "    \n",
    "    # Step 5: Perform a final fact-check on the recommendations\n",
    "    initial_answer = \" \".join([rec.page_content for rec in recommendations])\n",
    "    corrected_answer = final_fact_check(question, initial_answer, retriever, debug=debug)\n",
    "    \n",
    "    # Display the fact-checked answer for debugging\n",
    "    if debug:\n",
    "        print(f\"Corrected answer after final fact-check:\\n{corrected_answer}\")\n",
    "    \n",
    "    # Step 6: Decide and return the best answer\n",
    "    final_answer = decide_and_answer(question, retriever, debug=debug)\n",
    "    return final_answer\n",
    "\n",
    "def main():\n",
    "    # Set the data source and parameters for testing\n",
    "    data_source = \"Git\"  # Set to \"Local\" or \"Git\" as needed\n",
    "    question = \"Can you help me with how to make a streamlit app out of the data in the data section?\"\n",
    "    \n",
    "    # Define paths based on data source\n",
    "    if data_source == \"Local\":\n",
    "        repo_path = \"../../\"  # Example local directory path\n",
    "        repo_url = None\n",
    "    elif data_source == \"Git\":\n",
    "        repo_path = None\n",
    "        repo_url = \"https://github.com/ghadfield32/coach_analysis\"  # Example GitHub URL\n",
    "    else:\n",
    "        print(\"Invalid data source selected. Please choose 'Local' or 'Git'.\")\n",
    "        return\n",
    "    \n",
    "    # Run the RAG pipeline and print the final answer\n",
    "    try:\n",
    "        final_answer = run_rag_pipeline(data_source, repo_path=repo_path, repo_url=repo_url, question=question, debug=True)\n",
    "        print(\"\\nFinal Answer:\")\n",
    "        print(final_answer)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/git_repo_model/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/git_repo_model/app.py\n",
    "\n",
    "import streamlit as st\n",
    "from modules.decision_mechanism import decide_and_answer\n",
    "from modules.vector_store import create_vectorstore\n",
    "from modules.data_crawling import crawl_and_ingest\n",
    "from modules.git_data_crawling import load_github_repo\n",
    "from fact_checker import final_fact_check\n",
    "from modules.hyde_rag import contextual_retrieval  # Ensure we're importing contextual retrieval\n",
    "from modules.corrective_rag import corrective_rag\n",
    "\n",
    "def display_rag_guide():\n",
    "    \"\"\"Display an in-depth guide to RAG methods and their use in the pipeline.\"\"\"\n",
    "    st.markdown(\"## RAG Methods Explained\")\n",
    "    st.markdown(\"\"\"\n",
    "    **RAG (Retrieval-Augmented Generation)** is a powerful method for answering complex questions by retrieving relevant documents and generating responses. This app uses multiple RAG approaches, each carefully selected to enhance specific stages of the answer generation pipeline. Below is an overview of each step and the methods used:\n",
    "\n",
    "    ### 1. Data Crawling and Ingestion\n",
    "    - **Module**: `data_crawling.py` and `git_data_crawling.py`\n",
    "    - **Purpose**: This stage gathers documents from either a local directory or a GitHub repository. We support various file types (.py, .md, .csv, .ipynb, etc.) to capture a wide array of content. The documents are then processed and loaded into a format that’s ready for retrieval.\n",
    "    - **Why**: Comprehensive document crawling ensures a robust information base, allowing us to answer diverse questions based on specific data within your repository.\n",
    "\n",
    "    ### 2. Creating a Vector Store\n",
    "    - **Module**: `vector_store.py`\n",
    "    - **Purpose**: After gathering documents, they are embedded into a vector space using embeddings from Ollama’s `llama3.2` model. These embeddings are stored in a vector database (Chroma) for fast retrieval based on semantic similarity.\n",
    "    - **Why**: Embedding documents in a vector space makes it easier to identify the most relevant documents for a given question, enabling faster and more accurate retrieval.\n",
    "\n",
    "    ### 3. Contextual Retrieval\n",
    "    - **Module**: `contextual_retrieval.py`\n",
    "    - **Purpose**: Here, we enrich the content of each document by generating additional context, which provides a more nuanced basis for retrieval. Contextual Retrieval retrieves documents relevant to the question while enhancing them with extra contextual information to improve the accuracy of the generated answer.\n",
    "    - **Why**: Enriching each document with context allows the model to better understand and extract specific details relevant to the query. This approach improves retrieval precision, especially for complex queries.\n",
    "\n",
    "    ### 4. Corrective RAG (CRAG)\n",
    "    - **Module**: `corrective_rag.py`\n",
    "    - **Purpose**: This stage uses the retrieved documents to make specific recommendations or corrections based on their content. Corrective RAG reviews the initial answer for accuracy and coherence, then refines it by validating details against the retrieved documents.\n",
    "    - **Why**: By iteratively refining the initial response, CRAG ensures that the answer aligns well with the available data, creating a more accurate and trustworthy output.\n",
    "\n",
    "    ### 5. Self-RAG\n",
    "    - **Module**: `self_rag.py`\n",
    "    - **Purpose**: Self-RAG applies reflection on the initial response, critiquing and refining the answer. If the answer is found to need improvements, Self-RAG modifies the response accordingly, repeating this process for up to two reflections.\n",
    "    - **Why**: Self-refinement makes the response more robust and precise. By allowing the model to evaluate and adjust its output, Self-RAG helps in creating answers that are both accurate and concise.\n",
    "\n",
    "    ### 6. Web-Based Retrieval (Tavily Search)\n",
    "    - **Module**: `web_search.py`\n",
    "    - **Purpose**: When additional context is required, Tavily Search retrieves relevant information from the web. This step is integrated to provide external context that might complement the repository data, especially if the query requires a broader view.\n",
    "    - **Why**: Incorporating web-based retrieval ensures that the app can supplement internal data with up-to-date information from the web, enhancing response quality for more general or complex questions.\n",
    "\n",
    "    ### 7. Decision Mechanism\n",
    "    - **Module**: `decision_mechanism.py`\n",
    "    - **Purpose**: In this stage, confidence scores for each generated answer are evaluated, and the most reliable answer is selected. If the confidence scores are similar, the system combines elements from both answers for a balanced response.\n",
    "    - **Why**: A final decision mechanism selects the most appropriate response, ensuring the chosen answer is both relevant and trustworthy.\n",
    "\n",
    "    Each of these methods contributes uniquely to building answers with increased accuracy and adaptability, making this pipeline well-suited for navigating complex datasets and generating informed answers.\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "def run_rag_pipeline(data_source, repo_path=None, repo_url=None, question=\"What are the best practices for organizing a code repository?\", file_types=None, debug=False):\n",
    "    st.write(\"### Starting RAG pipeline...\")\n",
    "    st.write(f\"Data Source: {data_source}\")\n",
    "    st.write(f\"Question: {question}\")\n",
    "    st.write(f\"File Types: {file_types}\")\n",
    "\n",
    "    progress_bar = st.progress(0)\n",
    "\n",
    "    # Step 1: Load documents based on the selected data source\n",
    "    st.write(\"#### Step 1: Loading Documents...\")\n",
    "    if data_source == \"Local\":\n",
    "        if not repo_path:\n",
    "            raise ValueError(\"Local directory path must be provided for Local source.\")\n",
    "        documents = crawl_and_ingest(repo_path, file_types, debug)\n",
    "    elif data_source == \"Git\":\n",
    "        if not repo_url:\n",
    "            raise ValueError(\"GitHub repository URL must be provided for Git source.\")\n",
    "        documents = load_github_repo(repo_url, file_types, debug)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid data source selected. Choose either 'Local' or 'Git'.\")\n",
    "    \n",
    "    st.write(f\"Loaded {len(documents)} documents.\")\n",
    "    progress_bar.progress(0.2)\n",
    "\n",
    "    # Step 2: Create a vector store and a retriever\n",
    "    st.write(\"#### Step 2: Creating Vector Store and Retriever...\")\n",
    "    vectorstore = create_vectorstore(documents, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    st.write(\"Vector store created successfully.\")\n",
    "    progress_bar.progress(0.4)\n",
    "\n",
    "    # Step 3: Retrieve relevant documents using contextual retrieval\n",
    "    st.write(\"#### Step 3: Retrieving Relevant Documents (Contextual Retrieval)...\")\n",
    "    retrieved_docs = contextual_retrieval(question, retriever, debug)\n",
    "    st.write(f\"Retrieved {len(retrieved_docs)} relevant documents.\")\n",
    "    progress_bar.progress(0.6)\n",
    "\n",
    "    # Step 4: Generate recommendations using corrective RAG\n",
    "    st.write(\"#### Step 4: Generating Recommendations (Corrective RAG)...\")\n",
    "    recommendations = corrective_rag(retrieved_docs, debug=debug)\n",
    "    st.write(f\"Generated {len(recommendations)} recommendations.\")\n",
    "    progress_bar.progress(0.8)\n",
    "\n",
    "    # Step 5: Perform a final fact-check on the recommendations\n",
    "    st.write(\"#### Step 5: Performing Final Fact-Check...\")\n",
    "    initial_answer = \" \".join([rec.page_content for rec in recommendations])\n",
    "    corrected_answer = final_fact_check(question, initial_answer, retriever, debug=debug)\n",
    "    st.write(\"Fact-check completed.\")\n",
    "    progress_bar.progress(0.9)\n",
    "\n",
    "    # Step 6: Decide and return the best answer\n",
    "    st.write(\"#### Step 6: Finalizing the Best Answer...\")\n",
    "    final_answer = decide_and_answer(question, retriever, debug=debug)\n",
    "    st.write(\"Pipeline completed.\")\n",
    "    progress_bar.progress(1.0)\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "def main():\n",
    "    st.title(\"GitHub and Local Repository RAG Explorer\")\n",
    "    \n",
    "    # Option to display the RAG methods guide\n",
    "    if st.checkbox(\"Show RAG Methods Guide\"):\n",
    "        display_rag_guide()\n",
    "\n",
    "    data_source = st.radio(\"Select Data Source:\", [\"Local\", \"Git\"])\n",
    "    question = st.text_input(\"Enter your question:\", \"What are the best practices for organizing a code repository?\")\n",
    "    debug = st.checkbox(\"Enable Debug Mode\", value=False)\n",
    "\n",
    "    # File type filter selection\n",
    "    available_file_types = [\".py\", \".md\", \".csv\", \".ipynb\", \".html\", \".json\", \".yaml\", \".r\", \".cpp\", \".java\", \".scala\", \".sql\"]\n",
    "    file_types = st.multiselect(\"Select file types to include:\", available_file_types, default=available_file_types)\n",
    "\n",
    "    if data_source == \"Local\":\n",
    "        repo_path = st.text_input(\"Enter Local Directory Path:\")\n",
    "        repo_url = None\n",
    "    elif data_source == \"Git\":\n",
    "        repo_path = None\n",
    "        repo_url = st.text_input(\"Enter GitHub Repository URL:\")\n",
    "\n",
    "    if st.button(\"Run RAG Pipeline\"):\n",
    "        try:\n",
    "            final_answer = run_rag_pipeline(data_source, repo_path=repo_path, repo_url=repo_url, question=question, file_types=file_types, debug=debug)\n",
    "            st.write(\"### Final Answer:\")\n",
    "            st.write(final_answer)\n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
