{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG, RAG with Memory, Adaptive RAG, Corrective RAG, self-RAG, Agentive RAG... are you lost? Let me help you with this guide.\n",
    "\n",
    "1/ Simple RAG\n",
    "Retrieves relevant documents based on the query and uses them to generate an answer.\n",
    "\n",
    "2/ Simple RAG with Memory\n",
    "Extends Simple RAG by maintaining context from previous interactions.\n",
    "\n",
    "3/ Branched RAG\n",
    "Performs multiple retrieval steps, refining the search based on intermediate results.\n",
    "\n",
    "4/ HyDE (Hypothetical Document Embedding)\n",
    "Generates a hypothetical ideal document before retrieval to improve search relevance.\n",
    "\n",
    "5/ Adaptive RAG\n",
    "Dynamically adjusts retrieval and generation strategies based on the query type or difficulty.\n",
    "\n",
    "6/ Corrective RAG (CRAG)\n",
    "Iteratively refines generated responses by fact-checking against retrieved information.\n",
    "\n",
    "7/ Self-RAG\n",
    "The model critiques and improves its own responses using self-reflection and retrieval.\n",
    "\n",
    "8/ Agentic RAG\n",
    "Combines RAG with agentic behavior, allowing for more complex, multi-step problem-solving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.1/docs/get_started/quickstart/\n",
    "\n",
    "langchain quick start ^\n",
    "\n",
    "\n",
    "https://python.langchain.com/docs/integrations/providers/ollama/\n",
    "\n",
    "Ollama integrations ^\n",
    "\n",
    "Tool calling:\n",
    "https://ollama.com/blog/tool-support\n",
    "https://python.langchain.com/docs/how_to/tool_calling/\n",
    "\n",
    "\n",
    "- Easy example:\n",
    "https://github.com/Shubhamsaboo/awesome-llm-apps/blob/main/llama3.1_local_rag/llama3.1_local_rag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.0+cu124\n",
      "CUDA is available! GPU is ready to be used.\n",
      "Number of GPUs available: 1\n",
      "Current GPU: NVIDIA GeForce RTX 4090\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "  - Total Memory: 25.756696576 GB\n",
      "  - Compute Capability: (8, 9)\n",
      "Tensor on GPU: tensor([[0.7445, 0.2800, 0.9652],\n",
      "        [0.5206, 0.5027, 0.5118],\n",
      "        [0.6726, 0.8185, 0.4314]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Print the PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! GPU is ready to be used.\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. GPU is not set up correctly.\")\n",
    "\n",
    "# Print additional GPU details\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  - Total Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9} GB\")\n",
    "        print(f\"  - Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Create a random tensor and move it to the GPU\n",
    "    tensor = torch.rand(3, 3).cuda()\n",
    "    print(\"Tensor on GPU:\", tensor)\n",
    "else:\n",
    "    print(\"GPU is not available, cannot move tensor to GPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/custom_ollama_docker/notebooks/contextual_retreivel_rag/sports_news_rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Print the current working directory (optional for debugging)\n",
    "print(os.getcwd())\n",
    "\n",
    "# Set the path to your .env file relative to the current working directory\n",
    "dotenv_path = os.path.join(os.getcwd(), '../../.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "\n",
    "# Set up API keys\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/modules/data_crawling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/modules/data_crawling.py\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import concurrent.futures  # For parallel processing\n",
    "import os\n",
    "\n",
    "# Define the home directory for the project\n",
    "HOME_DIR = \"/workspaces/custom_ollama_docker\"\n",
    "\n",
    "def crawl_and_ingest(url, debug=False):\n",
    "    \"\"\"\n",
    "    Crawls a given URL, splits the document, generates propositions, \n",
    "    runs quality checks, and returns processed documents.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Crawling data from: {url}\")\n",
    "\n",
    "    # Load documents from the web URL\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Loaded {len(docs)} documents from {url}\")\n",
    "        print(f\"Document types loaded: {[type(doc) for doc in docs]}\")\n",
    "\n",
    "    # Split the documents into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    document_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Number of document chunks generated: {len(document_chunks)}\")\n",
    "\n",
    "    # Process each chunk to generate high-quality propositions\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(process_chunk, chunk, debug) for chunk in document_chunks]\n",
    "        processed_documents = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "    # Flatten the processed_documents to remove any nested list structures\n",
    "    proposition_documents = [doc for sublist in processed_documents for doc in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Total number of processed documents after flattening: {len(proposition_documents)}\")\n",
    "        print(f\"Types of processed documents: {[type(doc) for doc in proposition_documents]}\")\n",
    "\n",
    "    return proposition_documents\n",
    "\n",
    "\n",
    "def process_chunk(chunk, debug=False):\n",
    "    \"\"\"\n",
    "    Generates and quality checks propositions for a given chunk.\n",
    "    \"\"\"\n",
    "    propositions = generate_propositions(chunk.page_content, debug)\n",
    "    high_quality_propositions = quality_check_propositions(propositions, debug)\n",
    "    return [Document(page_content=prop) for prop in high_quality_propositions]\n",
    "\n",
    "def generate_propositions(text, debug=False):\n",
    "    \"\"\"\n",
    "    Generates propositions from the given text using an LLM.\n",
    "    \"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    max_length = 2000\n",
    "    text = text[:max_length] if len(text) > max_length else text\n",
    "\n",
    "    proposition_prompt = (\n",
    "        f\"Break down the following text into concise, complete, and meaningful factual statements:\\n\\n{text}\\n\\n\"\n",
    "        \"Provide each proposition as a separate statement.\"\n",
    "    )\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": proposition_prompt}]).content\n",
    "\n",
    "    propositions = [prop.strip() for prop in response.split('\\n') if prop.strip()]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Generated propositions: {propositions}\")\n",
    "\n",
    "    return propositions\n",
    "\n",
    "def quality_check_propositions(propositions, debug=False):\n",
    "    \"\"\"\n",
    "    Checks the quality of the propositions for accuracy, clarity, completeness, and conciseness.\n",
    "    \"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    high_quality_propositions = []\n",
    "\n",
    "    batch_size = 5\n",
    "    for i in range(0, len(propositions), batch_size):\n",
    "        batch = propositions[i:i + batch_size]\n",
    "        quality_prompt = (\n",
    "            f\"Evaluate the following propositions for accuracy, clarity, completeness, and conciseness. \"\n",
    "            f\"Score each aspect from 1 to 10 and provide an overall assessment. Reply with 'pass' if the proposition is acceptable:\\n\\n\"\n",
    "            f\"{', '.join(batch)}\"\n",
    "        )\n",
    "        response = llm.invoke([{\"role\": \"user\", \"content\": quality_prompt}]).content\n",
    "\n",
    "        results = response.lower().split('\\n')\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Batch being processed: {batch}\")\n",
    "            print(f\"LLM Response: {response}\")\n",
    "            print(f\"Number of results received: {len(results)}, Number of propositions in batch: {len(batch)}\")\n",
    "\n",
    "        min_length = min(len(results), len(batch))\n",
    "        for j in range(min_length):\n",
    "            if 'pass' in results[j]:\n",
    "                high_quality_propositions.append(batch[j])\n",
    "\n",
    "    return high_quality_propositions\n",
    "\n",
    "\n",
    "\n",
    "def main(debug=False):\n",
    "    # Sample sites for testing\n",
    "    sports_sites = [\"https://www.nba.com/\", \"https://www.espn.com/\"]\n",
    "    all_documents = []\n",
    "    for site in sports_sites:\n",
    "        documents = crawl_and_ingest(site, debug)\n",
    "        all_documents.extend(documents)\n",
    "    if debug:\n",
    "        print(f\"Total documents ingested: {len(all_documents)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/modules/vector_store.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/modules/vector_store.py\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "import os\n",
    "\n",
    "# Define the home directory for the project\n",
    "HOME_DIR = \"/workspaces/custom_ollama_docker\"\n",
    "DATA_DIR = os.path.join(HOME_DIR, \"data\", \"vectorstores\")\n",
    "\n",
    "def create_vectorstore(documents, site_name=\"nba\", debug=False):\n",
    "    # Specify embedding function\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    persist_directory = os.path.join(DATA_DIR, site_name)\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "    if debug:\n",
    "        print(f\"Creating vector store with {len(documents)} documents at {persist_directory}...\")\n",
    "\n",
    "    # Create Chroma vector store with embeddings\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "def create_pre_ingested_vectorstore(site_name, documents):\n",
    "    directory = f\"../../../data/vectorstores/{site_name.lower()}\"\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Initialize the embedding function\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    vectorstore = Chroma.from_documents(documents, embedding=embeddings, persist_directory=directory)\n",
    "    print(f\"Vector store for {site_name} created and saved at {directory}\")\n",
    "\n",
    "def main(debug=False):\n",
    "    # Use a list of high-quality Document objects instead of dictionaries\n",
    "    sample_docs = [Document(page_content=\"This is a high-quality sample document for testing.\")]\n",
    "    vectorstore = create_vectorstore(sample_docs, debug=debug)\n",
    "    if debug:\n",
    "        print(\"Vector store successfully created.\")\n",
    "        \n",
    "    # Example usage:\n",
    "    site_name = \"ESPN\"\n",
    "    documents = [Document(page_content=\"This is a sample document for NFL data.\")]\n",
    "    create_pre_ingested_vectorstore(site_name, documents)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/modules/contextual_retrieval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/modules/contextual_retrieval.py\n",
    "\n",
    "import copy\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "\n",
    "def create_contextual_nodes(documents, debug=False):\n",
    "    \"\"\"\n",
    "    Creates contextual nodes by enriching each document with additional context.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents (List[Document]): List of LangChain Document objects.\n",
    "    - debug (bool): Flag for printing debug information.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Document]: List of contextually enriched Document objects.\n",
    "    \"\"\"\n",
    "    # Initialize the LLM\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    \n",
    "    contextual_documents = []\n",
    "    for doc in documents:\n",
    "        # Generate contextual information using LLM\n",
    "        context_prompt = (\n",
    "            f\"Given the following document, generate contextual information that would help better understand its content:\\n\\n{doc.page_content}\\n\\n\"\n",
    "            \"Contextual information:\"\n",
    "        )\n",
    "        context = llm.invoke([{\"role\": \"user\", \"content\": context_prompt}]).content\n",
    "        \n",
    "        # Append the context to the document's metadata\n",
    "        enriched_doc = copy.deepcopy(doc)\n",
    "        enriched_doc.metadata[\"context\"] = context\n",
    "        contextual_documents.append(enriched_doc)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Generated context for document: {context}\")\n",
    "\n",
    "    return contextual_documents\n",
    "\n",
    "def create_embedding_retriever(documents, persist_directory='../../data/chroma_dbs', debug=False):\n",
    "    \"\"\"\n",
    "    Creates a Chroma vector store retriever using contextual nodes.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents (List[Document]): List of contextually enriched Document objects.\n",
    "    - persist_directory (str): Directory to persist the Chroma database.\n",
    "    - debug (bool): Flag for printing debug information.\n",
    "    \n",
    "    Returns:\n",
    "    - Chroma: Chroma vector store retriever object.\n",
    "    \"\"\"\n",
    "    # Create embeddings with Ollama\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    \n",
    "    # Create the Chroma vector store\n",
    "    if debug:\n",
    "        print(f\"Creating vector store with {len(documents)} contextually enriched documents...\")\n",
    "        \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Vector store created at {persist_directory}\")\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "def main(debug=True):\n",
    "    \"\"\"\n",
    "    Main function to test the contextual retrieval pipeline.\n",
    "    \"\"\"\n",
    "    # Sample documents for testing\n",
    "    sample_docs = [Document(page_content=\"The Boston Celtics won the NBA Finals in 2023.\")]\n",
    "    \n",
    "    # Create contextual nodes\n",
    "    contextual_docs = create_contextual_nodes(sample_docs, debug=debug)\n",
    "    \n",
    "    # Create and test the vector store\n",
    "    vectorstore = create_embedding_retriever(contextual_docs, debug=debug)\n",
    "    \n",
    "    # Output a message indicating successful creation of contextual retriever\n",
    "    if debug:\n",
    "        print(f\"Successfully created contextual retriever with {len(contextual_docs)} contextually enriched documents.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/modules/hyde_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/modules/hyde_rag.py\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from modules.contextual_retrieval import create_contextual_nodes, create_embedding_retriever\n",
    "from langchain.schema import Document\n",
    "\n",
    "def contextual_retrieval(question, retriever, debug=False):\n",
    "    \"\"\"\n",
    "    Performs contextual retrieval based on a given question and contextually enriched documents.\n",
    "    \n",
    "    Parameters:\n",
    "    - question (str): The query or question to retrieve documents for.\n",
    "    - retriever: The retriever object created from the contextual vector store.\n",
    "    - debug (bool): Flag for printing debug information.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Document]: List of retrieved documents based on the contextual retriever.\n",
    "    \"\"\"\n",
    "    # Generate a hypothetical answer to enrich the retrieval process\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    hypo_prompt = f\"Generate a detailed answer to the following question:\\n\\n{question}\\n\\nAnswer:\"\n",
    "    hypo_answer = llm.invoke([{\"role\": \"user\", \"content\": hypo_prompt}]).content\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Hypothetical answer generated: {hypo_answer}\")\n",
    "\n",
    "    # Retrieve documents using the contextual retriever\n",
    "    retrieved_docs = retriever.invoke(hypo_answer)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Number of documents retrieved based on hypothetical answer: {len(retrieved_docs)}\")\n",
    "        \n",
    "    return retrieved_docs\n",
    "\n",
    "def main(debug=False):\n",
    "    \"\"\"\n",
    "    Main function to test the contextual retrieval.\n",
    "    \"\"\"\n",
    "    question = \"What are the recent updates in the NBA?\"\n",
    "    \n",
    "    # Create a sample document\n",
    "    sample_docs = [Document(page_content=\"The Boston Celtics won the NBA Finals in 2023.\")]\n",
    "    \n",
    "    # Create contextual nodes and retriever\n",
    "    contextual_docs = create_contextual_nodes(sample_docs, debug=debug)\n",
    "    vectorstore = create_embedding_retriever(contextual_docs, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \n",
    "    # Test the contextual retrieval\n",
    "    contextual_retrieval(question, retriever, debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/modules/corrective_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/modules/corrective_rag.py\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document  # Import the Document class\n",
    "\n",
    "def corrective_rag(question, retrieved_docs, debug=False):\n",
    "    # Convert the list of dicts to Document objects if necessary\n",
    "    if not all(isinstance(doc, Document) for doc in retrieved_docs):\n",
    "        retrieved_docs = [Document(page_content=doc[\"page_content\"]) for doc in retrieved_docs]\n",
    "\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    initial_prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    initial_answer = llm.invoke([{\"role\": \"user\", \"content\": initial_prompt}]).content\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Initial answer generated: {initial_answer}\")\n",
    "\n",
    "    max_iterations = 2\n",
    "    for i in range(max_iterations):\n",
    "        verify_prompt = f\"Context: {context}\\n\\nAnswer: {initial_answer}\\n\\nIs the answer fully supported by the context? Identify any inaccuracies.\"\n",
    "        verification = llm.invoke([{\"role\": \"user\", \"content\": verify_prompt}]).content\n",
    "\n",
    "        if \"no inaccuracies\" in verification.lower():\n",
    "            if debug:\n",
    "                print(f\"No inaccuracies found. Answer is verified on iteration {i + 1}.\")\n",
    "            break\n",
    "        else:\n",
    "            refine_prompt = f\"Context: {context}\\n\\nThe initial answer may have inaccuracies: {verification}\\n\\nQuestion: {question}\\n\\nProvide a corrected answer:\"\n",
    "            initial_answer = llm.invoke([{\"role\": \"user\", \"content\": refine_prompt}]).content\n",
    "\n",
    "    return initial_answer\n",
    "\n",
    "def main(debug=False):\n",
    "    # Sample usage for testing\n",
    "    question = \"Who won the NBA Finals in 2023?\"\n",
    "    # Use a list of Document objects instead of dictionaries for the retrieved documents\n",
    "    retrieved_docs = [Document(page_content=\"The Boston Celtics won the NBA Finals in 2024.\")]\n",
    "    answer = corrective_rag(question, retrieved_docs, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"Final corrected answer: {answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/modules/self_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/modules/self_rag.py\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "def self_rag(question, initial_answer, debug=False):\n",
    "    \"\"\"Refine an initial answer by performing self-reflection and improvements.\"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    if debug:\n",
    "        print(f\"Initial answer before self-refinement: {initial_answer}\")\n",
    "    \n",
    "    max_reflections = 2  # Number of self-reflection iterations\n",
    "    for i in range(max_reflections):\n",
    "        # Self-reflection step\n",
    "        reflect_prompt = f\"Answer: {initial_answer}\\n\\nReflect on the answer and identify any areas for improvement.\"\n",
    "        reflection = llm.invoke([{\"role\": \"user\", \"content\": reflect_prompt}]).content\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Reflection result for iteration {i+1}: {reflection}\")\n",
    "\n",
    "        # If no improvements are needed, break out of the loop\n",
    "        if \"no improvements\" in reflection.lower():\n",
    "            if debug:\n",
    "                print(f\"No further improvements suggested after {i+1} iterations.\")\n",
    "            break\n",
    "        else:\n",
    "            # Improve the answer based on the reflection\n",
    "            improve_prompt = f\"Based on the reflection: {reflection}\\n\\nProvide an improved answer to the question: {question}\"\n",
    "            initial_answer = llm.invoke([{\"role\": \"user\", \"content\": improve_prompt}]).content\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Improved answer after iteration {i+1}: {initial_answer}\")\n",
    "\n",
    "    return initial_answer\n",
    "\n",
    "def main(debug=False):\n",
    "    # Sample usage for testing\n",
    "    question = \"What pick of the Draft was Bronny James Jr?\"\n",
    "    initial_answer = \"Bronny James Jr. was selected 55th\"\n",
    "    refined_answer = self_rag(question, initial_answer, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"Final refined answer: {refined_answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/modules/web_search.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/modules/web_search.py\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)\n",
    "\n",
    "def tavily_search(question, debug=False):\n",
    "    docs = tavily_retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join(f\"Source {i+1} ({doc.metadata.get('source')}):\\n{doc.page_content}\" for i, doc in enumerate(docs))\n",
    "    if debug:\n",
    "        print(f\"Web search context retrieved: {context[:500]}...\")  # Display first 500 chars\n",
    "    return context\n",
    "\n",
    "def main(debug=False):\n",
    "    question = \"Who was the first pick in the 2024 NBA Draft?\"\n",
    "    context = tavily_search(question, debug)\n",
    "    if debug:\n",
    "        print(f\"Retrieved context from Tavily search: {context}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/modules/decision_mechanism.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/modules/decision_mechanism.py\n",
    "from modules.hyde_rag import contextual_retrieval\n",
    "from modules.corrective_rag import corrective_rag\n",
    "from modules.web_search import tavily_search\n",
    "from modules.self_rag import self_rag  # Include the self_rag module for refinement\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.schema import Document\n",
    "\n",
    "def evaluate_confidence(answer, debug=False):\n",
    "    \"\"\"Evaluate the confidence of an answer using a language model.\"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    eval_prompt = (\n",
    "        f\"Evaluate the confidence level (on a scale of 1-10) of the following answer being correct, \"\n",
    "        f\"fully supported by reliable sources, and free from contradictions or inaccuracies:\\n\\n{answer}\\n\\n\"\n",
    "        \"Confidence Score:\"\n",
    "    )\n",
    "    confidence_score = llm.invoke([{\"role\": \"user\", \"content\": eval_prompt}]).content\n",
    "    try:\n",
    "        score = int(confidence_score.strip())\n",
    "    except ValueError:\n",
    "        score = 5  # Default to medium confidence if the evaluation fails\n",
    "    if debug:\n",
    "        print(f\"Confidence score evaluated: {score}\")\n",
    "    return score\n",
    "\n",
    "def decide_and_answer(question, retriever, progress_bar=None, progress_status=None, debug=False):\n",
    "    \"\"\"Generate answers using RAG and Tavily, and decide the best answer with self-refinement.\"\"\"\n",
    "    progress_step = 0.25\n",
    "\n",
    "    # Step 1: Use contextual retrieval to get documents and generate an initial RAG-based answer\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 1/4: Running HyDE retrieval...\")\n",
    "    retrieved_docs = contextual_retrieval(question, retriever, debug)\n",
    "    if progress_bar:\n",
    "        progress_bar.progress(progress_step)\n",
    "\n",
    "    # Step 2: Generate a corrective RAG-based answer\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 2/4: Generating a corrective RAG answer...\")\n",
    "    rag_answer = corrective_rag(question, retrieved_docs, debug)\n",
    "    rag_refined_answer = self_rag(question, rag_answer, debug)  # Refine RAG answer with self-rag\n",
    "    rag_confidence = evaluate_confidence(rag_refined_answer, debug)\n",
    "    progress_step += 0.25\n",
    "    if progress_bar:\n",
    "        progress_bar.progress(progress_step)\n",
    "\n",
    "    # Step 3: Use Tavily search to generate an answer\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 3/4: Running Tavily search for additional context...\")\n",
    "    tavily_context = tavily_search(question, debug)\n",
    "    tavily_prompt = f\"Context: {tavily_context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "    tavily_initial_answer = llm.invoke([{\"role\": \"user\", \"content\": tavily_prompt}]).content\n",
    "    tavily_refined_answer = self_rag(question, tavily_initial_answer, debug)  # Refine Tavily answer with self-rag\n",
    "    tavily_confidence = evaluate_confidence(tavily_refined_answer, debug)\n",
    "    progress_step += 0.25\n",
    "    if progress_bar:\n",
    "        progress_bar.progress(progress_step)\n",
    "\n",
    "    # Step 4: Decision mechanism to choose the final answer based on confidence scores\n",
    "    if progress_status:\n",
    "        progress_status.text(\"Step 4/4: Making the final decision...\")\n",
    "    if rag_confidence > tavily_confidence:\n",
    "        final_answer = rag_refined_answer\n",
    "        source = \"RAG-based response\"\n",
    "    elif tavily_confidence > rag_confidence:\n",
    "        final_answer = tavily_refined_answer\n",
    "        source = \"Tavily-based response\"\n",
    "    else:\n",
    "        # Combine answers if confidence scores are similar\n",
    "        combined_prompt = (\n",
    "            f\"Here are two potential answers to the question:\\n\\n\"\n",
    "            f\"Answer 1 (RAG-based):\\n{rag_refined_answer}\\n\\n\"\n",
    "            f\"Answer 2 (Tavily-based):\\n{tavily_refined_answer}\\n\\n\"\n",
    "            f\"Based on these, provide the best possible answer to the question: {question}\"\n",
    "        )\n",
    "        final_answer = llm.invoke([{\"role\": \"user\", \"content\": combined_prompt}]).content\n",
    "        source = \"Combined response\"\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Selected final answer from: {source}\")\n",
    "    return final_answer\n",
    "\n",
    "\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "def main(debug=False):\n",
    "    \"\"\"Main function to test the decision mechanism.\"\"\"\n",
    "    question = \"What pick of the draft was Bronny James?\"\n",
    "    \n",
    "    # Convert sample_docs into Document objects\n",
    "    sample_docs = [Document(page_content=\"This is a sample document for testing.\")]\n",
    "    vectorstore = create_vectorstore(sample_docs, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Create Streamlit progress bar and status\n",
    "    progress_bar = st.progress(0)  # Creates a Streamlit progress bar\n",
    "    progress_status = st.empty()  # Placeholder for status messages\n",
    "\n",
    "    # Pass these objects when calling decide_and_answer\n",
    "    final_answer = decide_and_answer(question, retriever, progress_bar, progress_status, debug)\n",
    "    st.write(f\"Final answer selected: {final_answer}\")  # Display the final answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/modules/fact_checker.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/modules/fact_checker.py\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from modules.hyde_rag import contextual_retrieval  # Import contextual_retrieval from the hyde_rag module\n",
    "from modules.web_search import tavily_search  # Import tavily_search from the web_search module\n",
    "from langchain.schema import Document\n",
    "\n",
    "def final_fact_check(question, answer, retriever, debug=False):\n",
    "    \"\"\"\n",
    "    Perform a final fact-check of the answer based on a combined context from retrieved documents and web search results.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): The question asked by the user.\n",
    "    answer (str): The initial answer generated by the RAG or web search.\n",
    "    retriever: The retriever object created from the vector store.\n",
    "    debug (bool): If True, print debug information.\n",
    "\n",
    "    Returns:\n",
    "    str: The fact-checked and potentially corrected answer.\n",
    "    \"\"\"\n",
    "    # Initialize the LLM for fact-checking\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "    # Retrieve documents using HyDE\n",
    "    retrieved_docs = contextual_retrieval(question, retriever, debug=debug)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs) if retrieved_docs else \"\"\n",
    "\n",
    "    # Retrieve web context using Tavily search\n",
    "    tavily_context = tavily_search(question, debug=debug)\n",
    "\n",
    "    # Combine both contexts\n",
    "    combined_context = context + \"\\n\\n\" + tavily_context\n",
    "\n",
    "    # Debug output for context combination\n",
    "    if debug:\n",
    "        print(f\"Combined context for fact-checking:\\n{combined_context}\")\n",
    "\n",
    "    # Create the fact-checking prompt\n",
    "    fact_check_prompt = (\n",
    "        f\"Context: {combined_context}\\n\\nAnswer: {answer}\\n\\n\"\n",
    "        f\"Verify the accuracy of the answer based on the context. Provide a corrected answer if necessary.\"\n",
    "    )\n",
    "\n",
    "    # Generate the fact-checked answer using the LLM\n",
    "    final_answer = llm.invoke([{\"role\": \"user\", \"content\": fact_check_prompt}]).content\n",
    "\n",
    "    # Debug output for final answer\n",
    "    if debug:\n",
    "        print(f\"Fact-checked answer: {final_answer}\")\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "def main(debug=False):\n",
    "    \"\"\"\n",
    "    Test the final_fact_check function with sample input.\n",
    "    \"\"\"\n",
    "    # Sample question and answer for testing\n",
    "    question = \"What pick of the Draft was Bronny James Jr?\"\n",
    "    initial_answer = \"Bronny James Jr. was selected by the Golden State Warriors with the 55th pick.\"  # Sample incorrect answer\n",
    "\n",
    "    # Use pre-loaded documents with Bronny James information\n",
    "    sample_docs = [Document(page_content=\"Bronny James was selected as the 55th pick in the 2024 NBA Draft.\")]\n",
    "    vectorstore = create_vectorstore(sample_docs, debug=debug)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # Run the final_fact_check function\n",
    "    corrected_answer = final_fact_check(question, initial_answer, retriever, debug=debug)\n",
    "    if debug:\n",
    "        print(f\"Corrected answer after final fact-check: {corrected_answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/main.py\n",
    "\n",
    "from modules.data_crawling import crawl_and_ingest\n",
    "from modules.vector_store import create_vectorstore\n",
    "from modules.decision_mechanism import decide_and_answer\n",
    "from modules.fact_checker import final_fact_check\n",
    "from modules.hyde_rag import contextual_retrieval  # Use the new contextual retrieval function\n",
    "\n",
    "def main(debug=False):\n",
    "    # Define test sites, including new ones\n",
    "    sports_sites = [\"https://www.nba.com/\", \"https://www.espn.com/\", \"https://www.nfl.com/\"]\n",
    "\n",
    "    all_documents = []\n",
    "\n",
    "    # Step 1: Crawl and ingest data from test sites\n",
    "    for site in sports_sites:\n",
    "        documents = crawl_and_ingest(site, debug)\n",
    "        \n",
    "        # Confirm each document type and content after ingestion\n",
    "        if debug:\n",
    "            print(f\"Documents from {site}: {[type(doc) for doc in documents]}\")\n",
    "            print(f\"Number of documents ingested from {site}: {len(documents)}\")\n",
    "            for doc in documents[:3]:  # Print first few documents as a sample\n",
    "                print(f\"Sample content from {site}: {doc.page_content[:500]}...\")  # Show the first 500 chars for brevity\n",
    "        \n",
    "        all_documents.extend(documents)\n",
    "\n",
    "    # Flatten list in case of nested lists\n",
    "    all_documents = [doc for doc in all_documents if isinstance(doc, Document)]\n",
    "    \n",
    "    # Step 2: Create vector store from ingested documents\n",
    "    if debug:\n",
    "        print(f\"Total documents after flattening: {len(all_documents)}\")\n",
    "\n",
    "    if all_documents:\n",
    "        vectorstore = create_vectorstore(all_documents, debug=debug)\n",
    "        retriever = vectorstore.as_retriever()\n",
    "\n",
    "        # Step 3: Ask a sample question and check the answer generation\n",
    "        question = \"What pick of the Draft was Bronny James jr?\"\n",
    "        initial_answer = contextual_retrieval(question, retriever, debug)\n",
    "\n",
    "        # Step 4: Fact-check and print the answer\n",
    "        final_answer = final_fact_check(question, initial_answer, retriever, debug)\n",
    "        print(f\"Final answer for the question '{question}': {final_answer}\")\n",
    "    else:\n",
    "        print(\"No documents were ingested, please check the crawl_and_ingest function for errors with the selected sites.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../src/sports_news_rag/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../src/sports_news_rag/app.py\n",
    "import streamlit as st\n",
    "from modules.decision_mechanism import decide_and_answer, evaluate_confidence  # Import evaluate_confidence\n",
    "from modules.vector_store import create_vectorstore\n",
    "from modules.data_crawling import crawl_and_ingest\n",
    "from modules.fact_checker import final_fact_check\n",
    "from modules.hyde_rag import contextual_retrieval\n",
    "from modules.corrective_rag import corrective_rag\n",
    "from modules.self_rag import self_rag\n",
    "from modules.web_search import tavily_search  # Import tavily_search\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "# Set up the Streamlit app title and description (MUST be the first Streamlit command)\n",
    "st.set_page_config(page_title=\"Advanced Sports News RAG Bot\", layout=\"wide\")\n",
    "\n",
    "# Debug: Confirm the current working directory\n",
    "current_working_dir = os.getcwd()\n",
    "st.sidebar.write(f\"Current Working Directory: {current_working_dir}\")\n",
    "\n",
    "# Load environment variables\n",
    "dotenv_path = os.path.join(current_working_dir, '.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Define the missing function to generate an answer from context\n",
    "def generate_answer_from_context(context, question, debug=False):\n",
    "    \"\"\"\n",
    "    Generate an answer based on the provided context and user question.\n",
    "    \n",
    "    Parameters:\n",
    "    - context (str): The text context retrieved from Tavily search.\n",
    "    - question (str): The user's question to answer.\n",
    "    - debug (bool): If True, enables debug output.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The generated answer based on the context.\n",
    "    \"\"\"\n",
    "    llm = ChatOllama(model=\"llama3.2\", temperature=0)  # Ensure the same LLM model is used\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    response = llm.invoke([{\"role\": \"user\", \"content\": prompt}]).content\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Generated answer from context: {response}\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Confirm environment path and any loaded variables\n",
    "if os.path.exists(dotenv_path):\n",
    "    st.sidebar.write(f\".env file found at: {dotenv_path}\")\n",
    "else:\n",
    "    st.sidebar.write(f\".env file not found at: {dotenv_path}\")\n",
    "\n",
    "st.title(\"Advanced Sports News RAG Bot\")\n",
    "st.write(\"Get the most up-to-date sports news using advanced RAG techniques. This bot combines information from various sources and fact-checks responses for reliability.\")\n",
    "\n",
    "# Adding the introduction tab\n",
    "tabs = st.tabs([\"Introduction\", \"Ask a Question\"])\n",
    "\n",
    "# Introduction tab content\n",
    "with tabs[0]:\n",
    "    st.header(\"Approaches Used in Advanced Versatile RAG Bot\")\n",
    "    st.write(\"\"\"\n",
    "    This project leverages a variety of Retrieval-Augmented Generation (RAG) strategies to create an interactive assistant capable of providing reliable, up-to-date information for any type of website, though it has been initially applied to sports news. Below, we detail the approaches utilized, how they contribute to the quality of answers, and the innovative combination of different RAG methodologies.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Simple RAG\")\n",
    "    st.write(\"\"\"\n",
    "    Simple RAG forms the foundation of our bot by retrieving documents based on a user query and generating grounded answers with a large language model (LLM). It minimizes hallucination issues by anchoring the generated responses to relevant sources.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Branched RAG\")\n",
    "    st.write(\"\"\"\n",
    "    Our Branched RAG approach performs multiple retrieval layers, refining searches based on intermediate results. This iterative process enhances answer specificity and is especially valuable for complex or multi-layered queries.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Contextual Retrieval\")\n",
    "    st.write(\"\"\"\n",
    "    Contextual Retrieval replaces hypothetical document generation (HyDE) by enriching each document with additional context, making retrieval results more aligned with nuanced queries. By adding contextual nodes to documents, the bot achieves higher precision and recall in retrieving relevant data.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Corrective RAG (CRAG)\")\n",
    "    st.write(\"\"\"\n",
    "    Corrective RAG (CRAG) iteratively checks and refines responses by comparing them to the context from retrieved documents. This ensures answers are accurate and well-supported, enhancing reliability and factual correctness.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Self-RAG\")\n",
    "    st.write(\"\"\"\n",
    "    Self-RAG adds a layer of self-reflection where the model re-evaluates its initial answer to make improvements in clarity, conciseness, and accuracy. This self-assessment strengthens response quality by ensuring coherence and completeness.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Agentic RAG\")\n",
    "    st.write(\"\"\"\n",
    "    Agentic RAG orchestrates multi-step queries, allowing the bot to act as an autonomous agent. By combining retrieval, verification, and synthesis processes, Agentic RAG enables intelligent navigation of information sources, crafting answers that involve interconnected insights.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Tavily Web Search\")\n",
    "    st.write(\"\"\"\n",
    "    Tavily Web Search complements RAG by dynamically searching the web for up-to-date information, especially when pre-ingested documents do not fully address a query. This integration ensures the bot’s responses reflect the latest information available.\n",
    "    \"\"\")\n",
    "\n",
    "    st.subheader(\"Final Fact-Check\")\n",
    "    st.write(\"\"\"\n",
    "    As a final step, the bot performs a comprehensive fact-check using combined contexts from both pre-ingested documents and Tavily search results. This step verifies that the answer provided aligns with reliable, current information, enhancing trustworthiness.\n",
    "    \"\"\")\n",
    "\n",
    "    st.header(\"How These Approaches Work Together\")\n",
    "    st.write(\"\"\"\n",
    "    By integrating these RAG methods, our bot achieves high accuracy, adaptability, and domain versatility. Here’s how they work in tandem:\n",
    "\n",
    "    - **Initial Retrieval**: Simple RAG retrieves documents relevant to the query.\n",
    "    - **Refinement**: Branched RAG and Contextual Retrieval enhance document selection, providing more precise data.\n",
    "    - **Verification**: Corrective RAG verifies factual accuracy, and Self-RAG refines answer quality.\n",
    "    - **Dynamic Updates**: Tavily Search supplements with current web-based information when needed.\n",
    "    - **Multi-step Processing**: Agentic RAG manages complex queries requiring multiple information sources.\n",
    "    - **Final Fact-Check**: Ensures responses are reliable and up-to-date, combining all contexts effectively.\n",
    "\n",
    "    These methods together create a robust, adaptive assistant capable of providing clear, reliable answers to dynamic questions.\n",
    "    \"\"\")\n",
    "\n",
    "    st.header(\"The Value of Combined RAG Approaches\")\n",
    "    st.write(\"\"\"\n",
    "    By integrating these techniques, our system is capable of:\n",
    "\n",
    "    - **High accuracy**: Corrective checks ensure factual answers.\n",
    "    - **Adaptability**: Contextual enhancement bridges knowledge gaps.\n",
    "    - **Depth in retrieval**: Branched and Agentic RAGs enable nuanced understanding.\n",
    "    - **Domain versatility**: Capable of handling various domains beyond sports.\n",
    "    - **Real-time information**: Tavily Search provides the latest web updates.\n",
    "    - **Context retention**: Maintains relevant context across user interactions for a more interactive experience.\n",
    "\n",
    "    These combined approaches make the Versatile RAG Bot capable of not only providing reliable answers but also refining and adapting outputs intelligently across a range of queries.\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# Ask a Question tab content\n",
    "with tabs[1]:\n",
    "    # Sidebar configuration options\n",
    "    st.sidebar.title(\"Configuration\")\n",
    "    enable_debug = st.sidebar.checkbox(\"Enable Debugging\", value=False)\n",
    "    include_fact_check = st.sidebar.checkbox(\"Include Final Fact-Check\", value=True)\n",
    "    use_pre_ingested_data = st.sidebar.checkbox(\"Use Pre-Ingested Data\", value=True)\n",
    "\n",
    "    # Dynamic Source Selection for Fact-Checking\n",
    "    fact_check_sources = st.sidebar.text_input(\n",
    "        \"Enter Custom URLs for Fact-Checking (comma-separated)\", \n",
    "        \"https://www.nba.com, https://www.espn.com\"\n",
    "    )\n",
    "\n",
    "    # Ensure the session state attributes are initialized\n",
    "    if \"vectorstore\" not in st.session_state:\n",
    "        st.session_state.vectorstore = None\n",
    "    if \"retriever\" not in st.session_state:\n",
    "        st.session_state.retriever = None\n",
    "    if \"all_documents\" not in st.session_state:\n",
    "        st.session_state.all_documents = []\n",
    "\n",
    "    # Load or ingest data\n",
    "    if use_pre_ingested_data:\n",
    "        st.sidebar.subheader(\"Pre-Ingested Data Loading\")\n",
    "        known_sites = [\"NBA\", \"ESPN\", \"NFL\"]\n",
    "        selected_site = st.sidebar.selectbox(\"Select Pre-Ingested Site:\", known_sites)\n",
    "        \n",
    "        if st.sidebar.button(\"Load Pre-Ingested Data\"):\n",
    "            with st.spinner(f\"Loading pre-ingested data for {selected_site}...\"):\n",
    "                pre_ingested_vectorstore_path = os.path.join(current_working_dir, f\"data/vectorstores/{selected_site.lower()}\")\n",
    "\n",
    "                # Ensure the embedding function is specified\n",
    "                embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "                if os.path.exists(pre_ingested_vectorstore_path):\n",
    "                    try:\n",
    "                        # Include embedding function in Chroma initialization\n",
    "                        st.session_state.vectorstore = Chroma(\n",
    "                            persist_directory=pre_ingested_vectorstore_path,\n",
    "                            embedding_function=embeddings\n",
    "                        )\n",
    "                        st.session_state.retriever = st.session_state.vectorstore.as_retriever()\n",
    "                        st.sidebar.success(f\"Loaded pre-ingested data for {selected_site}.\")\n",
    "                    except Exception as e:\n",
    "                        st.sidebar.error(f\"Error loading pre-ingested data for {selected_site}: {str(e)}\")\n",
    "                else:\n",
    "                    st.sidebar.error(f\"Pre-ingested data for {selected_site} not found at path: {pre_ingested_vectorstore_path}\")\n",
    "                    \n",
    "    else:\n",
    "        # Allow the user to input custom URLs\n",
    "        custom_sports_sites = st.sidebar.text_input(\n",
    "            \"Enter custom URLs for crawling data (comma-separated)\",\n",
    "            \"https://www.nba.com, https://www.espn.com\"\n",
    "        ).split(\",\")\n",
    "\n",
    "        if st.sidebar.button(\"Ingest Data\"):\n",
    "            with st.spinner(\"Crawling and ingesting data...\"):\n",
    "                st.session_state.all_documents = []\n",
    "                for site in custom_sports_sites:\n",
    "                    site = site.strip()\n",
    "                    if site:\n",
    "                        documents = crawl_and_ingest(site, debug=enable_debug)\n",
    "                        st.session_state.all_documents.extend(documents)\n",
    "                st.sidebar.success(f\"Data ingested from {len(custom_sports_sites)} sites.\")\n",
    "\n",
    "        if st.sidebar.button(\"Create Vector Store\"):\n",
    "            with st.spinner(\"Creating vector store from dynamically ingested data...\"):\n",
    "                if st.session_state.all_documents:\n",
    "                    st.session_state.vectorstore = create_vectorstore(st.session_state.all_documents, debug=enable_debug)\n",
    "                    st.session_state.retriever = st.session_state.vectorstore.as_retriever()\n",
    "                    st.sidebar.success(\"Vector store created and retriever set up.\")\n",
    "                else:\n",
    "                    st.sidebar.error(\"No documents available. Please ingest data first.\")\n",
    "\n",
    "    # User question input\n",
    "    st.subheader(\"Ask a Sports-Related Question\")\n",
    "    user_question = st.text_input(\"Enter your question about sports news or events:\", \"What pick of the Draft was Bronny James Jr?\")\n",
    "\n",
    "    if st.button(\"Get Answer\"):\n",
    "        if not st.session_state.retriever:\n",
    "            st.error(\"Please load or create the vector store first.\")\n",
    "        elif user_question:\n",
    "            # Initialize progress bar\n",
    "            progress_bar = st.progress(0)\n",
    "            progress_status = st.empty()\n",
    "\n",
    "            with st.spinner(\"Starting RAG process...\"):\n",
    "                # Step 1: Initial Retrieval and Hypothetical Document Generation\n",
    "                progress_status.text(\"Step 1: Performing Initial Contextual Retrieval...\")\n",
    "                retrieved_docs = contextual_retrieval(user_question, st.session_state.retriever, debug=enable_debug)\n",
    "                progress_bar.progress(0.25)\n",
    "                \n",
    "                if enable_debug:\n",
    "                    st.write(f\"Contextual Retrieval Output: {retrieved_docs[:2]}\")  # Display first 2 for brevity\n",
    "\n",
    "                # Step 2: Corrective RAG\n",
    "                progress_status.text(\"Step 2: Generating Corrective RAG Answer...\")\n",
    "                rag_answer = corrective_rag(user_question, retrieved_docs, debug=enable_debug)\n",
    "                progress_bar.progress(0.5)\n",
    "\n",
    "                # Step 3: Self-Refinement on RAG Answer\n",
    "                progress_status.text(\"Step 3: Refining RAG Answer with Self-RAG...\")\n",
    "                rag_refined_answer = self_rag(user_question, rag_answer, debug=enable_debug)\n",
    "                rag_confidence = evaluate_confidence(rag_refined_answer, debug=enable_debug)\n",
    "                progress_bar.progress(0.6)\n",
    "                \n",
    "                if enable_debug:\n",
    "                    st.write(f\"Refined RAG Answer: {rag_refined_answer}\")\n",
    "                    st.write(f\"RAG Confidence Score: {rag_confidence}\")\n",
    "\n",
    "                # Step 4: External Tavily Search\n",
    "                progress_status.text(\"Step 4: Performing Tavily Web Search...\")\n",
    "                tavily_context = tavily_search(user_question, debug=enable_debug)\n",
    "                tavily_answer = generate_answer_from_context(tavily_context, user_question)\n",
    "                progress_bar.progress(0.8)\n",
    "\n",
    "                # Self-RAG on Tavily answer\n",
    "                progress_status.text(\"Refining Tavily Answer with Self-RAG...\")\n",
    "                tavily_refined_answer = self_rag(user_question, tavily_answer, debug=enable_debug)\n",
    "                tavily_confidence = evaluate_confidence(tavily_refined_answer, debug=enable_debug)\n",
    "                progress_bar.progress(0.9)\n",
    "                \n",
    "                if enable_debug:\n",
    "                    st.write(f\"Tavily Answer: {tavily_refined_answer}\")\n",
    "                    st.write(f\"Tavily Confidence Score: {tavily_confidence}\")\n",
    "\n",
    "                # Step 5: Decision Mechanism\n",
    "                progress_status.text(\"Step 5: Making Final Decision...\")\n",
    "                if rag_confidence > tavily_confidence:\n",
    "                    final_answer = rag_refined_answer\n",
    "                    source = \"RAG-based response\"\n",
    "                elif tavily_confidence > rag_confidence:\n",
    "                    final_answer = tavily_refined_answer\n",
    "                    source = \"Tavily-based response\"\n",
    "                else:\n",
    "                    # Combine answers if confidence scores are similar\n",
    "                    combined_prompt = (\n",
    "                        f\"Here are two potential answers to the question:\\n\\n\"\n",
    "                        f\"Answer 1 (RAG-based):\\n{rag_refined_answer}\\n\\n\"\n",
    "                        f\"Answer 2 (Tavily-based):\\n{tavily_refined_answer}\\n\\n\"\n",
    "                        f\"Based on these, provide the best possible answer to the question: {user_question}\"\n",
    "                    )\n",
    "                    llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "                    final_answer = llm.invoke([{\"role\": \"user\", \"content\": combined_prompt}]).content\n",
    "                    source = \"Combined response\"\n",
    "                progress_bar.progress(1.0)\n",
    "                \n",
    "                if enable_debug:\n",
    "                    st.write(f\"Final Answer Selected from {source}: {final_answer}\")\n",
    "\n",
    "                # Optional fact-check with custom sources (sources parameter removed here)\n",
    "                if include_fact_check:\n",
    "                    progress_status.text(\"Performing final fact-check...\")\n",
    "                    final_answer = final_fact_check(user_question, final_answer, st.session_state.retriever, debug=enable_debug)\n",
    "                    progress_bar.progress(1.0)\n",
    "\n",
    "                # Display final answer\n",
    "                st.subheader(\"Answer\")\n",
    "                st.write(final_answer)\n",
    "        else:\n",
    "            st.error(\"Please enter a question.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
