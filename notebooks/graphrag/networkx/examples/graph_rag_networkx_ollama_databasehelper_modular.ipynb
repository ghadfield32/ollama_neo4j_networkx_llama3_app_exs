{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free Ollama GraphRag with Llama 3.2 currently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by: https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques_runnable_scripts/graph_rag.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Graph-based System for Database Understanding and Analysis\n",
    "\n",
    "Creating a system that leverages a knowledge graph to understand and interact with databases is an excellent application of the `GraphRAG` framework. This system can assist in understanding existing database structures, answering questions related to schema design, and helping identify duplicates or inefficiencies in the data. Here’s a deep dive into how this could be built:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Problem Definition and Use Cases**\n",
    "\n",
    "The goal is to create a graph-based system that ingests database schemas, table relationships, and metadata to:\n",
    "\n",
    "1. **Understand Database Structures**:\n",
    "   - Provide insights into the overall structure of the database.\n",
    "   - Highlight relationships between tables and columns.\n",
    "   - Visualize database schemas and dependencies.\n",
    "\n",
    "2. **Assist in Schema Design**:\n",
    "   - Suggest new tables based on existing data.\n",
    "   - Recommend optimal schema designs for new data that needs to be added.\n",
    "\n",
    "3. **Identify Duplicates and Redundant Information**:\n",
    "   - Detect tables or columns with similar purposes.\n",
    "   - Recommend merging strategies for redundant tables.\n",
    "   - Identify and highlight data duplication issues.\n",
    "\n",
    "4. **Answer Complex Database-related Queries**:\n",
    "   - Handle questions like, “How can I create a new table using the `Sales` and `Product` tables?” or “What are the dependencies for the `Order` table?”\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Understanding the System Architecture**\n",
    "\n",
    "The architecture can be broken down into several core components, each responsible for a distinct task:\n",
    "\n",
    "#### 2.1 **Database Ingestion and Metadata Processing**\n",
    "- **Input:**\n",
    "  - Database schemas, table relationships, column descriptions, and metadata.\n",
    "  - Optionally, sample data can be used to understand the distribution and relationships within columns.\n",
    "\n",
    "- **Processing:**\n",
    "  - Parse the schema files or connect directly to the database to extract metadata such as table names, columns, data types, constraints, primary keys, foreign keys, and indices.\n",
    "  - Extract inter-table relationships and dependencies.\n",
    "\n",
    "- **Output:**\n",
    "  - A structured representation of the database in a graph format, where:\n",
    "    - **Nodes** represent tables, columns, and constraints.\n",
    "    - **Edges** represent relationships such as foreign keys, joins, or dependencies.\n",
    "\n",
    "#### 2.2 **Graph Construction and Concept Mapping**\n",
    "- Use a **graph database** (e.g., Neo4j) or an in-memory graph library (e.g., `networkx`) to build the knowledge graph:\n",
    "  - Each table is a node with properties like table name, number of rows, and table description.\n",
    "  - Each column is a sub-node or an attribute of a table node, with properties like column type, constraints, and indices.\n",
    "  - Relationships (edges) indicate joins or foreign key dependencies, with weights assigned based on their strength or frequency of use in queries.\n",
    "\n",
    "#### 2.3 **Query Engine for Schema and Data Analysis**\n",
    "- Implement a **Query Engine** that:\n",
    "  - Can interpret complex database-related questions (e.g., \"How do I join the `Sales` table with the `Customer` table?\").\n",
    "  - Suggests optimal joins or schema modifications based on existing data and metadata.\n",
    "  - Generates new SQL queries or schema suggestions.\n",
    "\n",
    "#### 2.4 **Analysis and Deduplication Module**\n",
    "- This module will leverage graph traversal techniques and similarity measures to identify:\n",
    "  - Duplicates of tables or columns based on data content, column names, and use cases.\n",
    "  - Recommendations for merging or restructuring tables.\n",
    "\n",
    "#### 2.5 **Visualization and Interactive Exploration**\n",
    "- The visualization component should:\n",
    "  - Use graph-based visualization tools like `networkx`, `plotly`, or web-based frameworks like D3.js.\n",
    "  - Provide an interactive interface for exploring table relationships, schema structures, and dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Deep Dive into Key Concepts**\n",
    "\n",
    "To understand how each component contributes to the overall system, let’s dive into some core ideas and their implementation details:\n",
    "\n",
    "#### 3.1 **Graph Construction for Database Representation**\n",
    "- **Nodes**:\n",
    "  - Table nodes contain metadata such as table name, description, and constraints.\n",
    "  - Column nodes include data type, indices, and constraints.\n",
    "  - Constraint nodes (e.g., primary keys) and relationships (e.g., joins) are first-class entities in the graph to represent dependency paths.\n",
    "\n",
    "- **Edges**:\n",
    "  - Direct relationships between tables (e.g., foreign key relationships) can be weighted based on the number of joins or queries involving these tables.\n",
    "  - Semantic relationships between columns (e.g., columns storing similar data across tables) can be used to suggest potential schema merging or reorganization.\n",
    "\n",
    "#### 3.2 **Leveraging Embeddings and Similarity Analysis for Columns and Tables**\n",
    "- Use column and table metadata to generate **vector embeddings**:\n",
    "  - For each column, consider features like data type, column name, and constraints to create a feature vector.\n",
    "  - For each table, use the embeddings of its columns, aggregated using methods like mean or attention-based pooling, to create a table embedding.\n",
    "\n",
    "- **Similarity Analysis**:\n",
    "  - Compare column and table embeddings to identify redundancies or similar entities.\n",
    "  - Use cosine similarity or other distance measures to detect potential duplicates or suggest schema modifications.\n",
    "\n",
    "#### 3.3 **Handling Queries for Schema Suggestions and Modifications**\n",
    "- Implement a query parser that understands database-related questions:\n",
    "  - Use natural language processing to interpret the query and map it to database entities (e.g., \"Which columns are similar to `CustomerID` in the `Order` table?\" can be mapped to searching for columns with similar names or data types).\n",
    "\n",
    "- For complex queries, implement a graph traversal algorithm to:\n",
    "  - Trace dependencies and joins between tables.\n",
    "  - Suggest new schema elements or query modifications based on existing relationships.\n",
    "\n",
    "#### 3.4 **Deduplication and Schema Optimization Module**\n",
    "- This module will focus on:\n",
    "  - **Column Duplication**: Identify columns that serve similar purposes across different tables. E.g., if `CustomerName` exists in both `Orders` and `Sales` tables, consider centralizing it.\n",
    "  - **Table Deduplication**: Identify tables with overlapping purposes or contents. Suggest merging or restructuring strategies.\n",
    "  - **Unused or Rarely Accessed Tables**: Highlight tables or columns that are infrequently used in queries or have limited relevance.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Detailed Implementation Strategy**\n",
    "\n",
    "Let’s look at a high-level plan to implement this system step-by-step:\n",
    "\n",
    "1. **Database Schema Ingestion:**\n",
    "   - Connect to the database using a standard library like `sqlalchemy` or directly parse the schema definition files (e.g., `.sql` or `.dbml` files).\n",
    "   - Create initial nodes for each table and their columns with metadata as node attributes.\n",
    "\n",
    "2. **Graph Construction:**\n",
    "   - Use a library like `networkx` to represent the schema as a graph.\n",
    "   - For each table, create nodes for the table and its columns.\n",
    "   - For each relationship (e.g., foreign key), create an edge between table nodes.\n",
    "\n",
    "3. **Concept Extraction and Embedding:**\n",
    "   - Use a language model or text-based embeddings to capture the semantics of table and column names.\n",
    "   - Build table and column embeddings, store them as node attributes.\n",
    "\n",
    "4. **Query Handling and Traversal:**\n",
    "   - Create a query engine that leverages graph traversal to answer schema-related questions.\n",
    "   - Implement context expansion techniques (similar to GraphRAG) to traverse through related nodes and gather context for answering complex questions.\n",
    "\n",
    "5. **Deduplication Analysis:**\n",
    "   - Use graph-based similarity measures to detect duplicates and redundancies.\n",
    "   - Implement a scoring mechanism to suggest merging or restructuring.\n",
    "\n",
    "6. **Visualization and Interaction:**\n",
    "   - Use `matplotlib`, `plotly`, or a web-based graph visualization library to provide an interactive interface.\n",
    "   - Allow users to query, explore, and understand the database schema through an intuitive graphical interface.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Challenges and Considerations**\n",
    "\n",
    "1. **Handling Large Databases**: If the database has thousands of tables and millions of columns, the graph can become very large. Strategies like lazy loading, selective traversal, and graph pruning will be needed.\n",
    "\n",
    "2. **Metadata Consistency**: Ensuring that metadata is up-to-date and correctly represents the database can be challenging, especially if the schema changes frequently.\n",
    "\n",
    "3. **Understanding Semantics**: Automatically understanding the purpose of columns or tables based on just names and types can be difficult. Advanced language models or knowledge bases might be required to enhance semantic understanding.\n",
    "\n",
    "4. **Performance Optimization**: Traversing large graphs and performing complex similarity calculations can be resource-intensive. Efficient data structures and parallel processing might be needed.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Future Extensions**\n",
    "\n",
    "- **Integrate with Real-time Query Analysis**: Capture live query logs and update the graph in real-time based on query patterns and table usage.\n",
    "- **Leverage Machine Learning for Schema Recommendations**: Use historical data and query patterns to train models that can suggest schema optimizations.\n",
    "- **Add Support for Multiple Databases**: Handle scenarios where multiple databases need to be analyzed together, such as in a data warehouse or federated database scenario.\n",
    "\n",
    "---\n",
    "\n",
    "This system can be a powerful tool for organizations looking to streamline their data management processes, optimize schema designs, and gain deeper insights into their database structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. database_setup.py\n",
    "\n",
    "This module handles database creation and table setup, initializing two SQLite databases and returning their engines and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../../src/database_optimizer_networkx_rag_llama3/modules/database_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../../src/database_optimizer_networkx_rag_llama3/modules/database_setup.py\n",
    "# database_setup.py\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, ForeignKey\n",
    "import os\n",
    "\n",
    "# Define the fixed path for example databases\n",
    "DB_PATH = '/workspaces/custom_ollama_docker/data/graph_chroma_dbs/networkx'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(DB_PATH, exist_ok=True)\n",
    "\n",
    "def setup_databases(debug=False):\n",
    "    engine1 = create_engine(f'sqlite:///{os.path.join(DB_PATH, \"example1.db\")}')\n",
    "    engine2 = create_engine(f'sqlite:///{os.path.join(DB_PATH, \"example2.db\")}')\n",
    "    metadata1 = MetaData()\n",
    "    metadata2 = MetaData()\n",
    "\n",
    "    # Define tables for the first database\n",
    "    Table('employees', metadata1, Column('id', Integer, primary_key=True), Column('name', String),\n",
    "          Column('department', String), Column('salary', Integer), Column('manager_id', Integer, ForeignKey('managers.id')))\n",
    "    Table('managers', metadata1, Column('id', Integer, primary_key=True), Column('name', String),\n",
    "          Column('department', String))\n",
    "    Table('departments', metadata1, Column('id', Integer, primary_key=True), Column('name', String),\n",
    "          Column('location', String))\n",
    "\n",
    "    # Define tables for the second database\n",
    "    Table('staff', metadata2, Column('id', Integer, primary_key=True), Column('full_name', String),\n",
    "          Column('dept', String), Column('wage', Integer), Column('supervisor_id', Integer, ForeignKey('supervisors.id')))\n",
    "    Table('supervisors', metadata2, Column('id', Integer, primary_key=True), Column('full_name', String),\n",
    "          Column('dept', String))\n",
    "    Table('office_locations', metadata2, Column('id', Integer, primary_key=True), Column('office_name', String),\n",
    "          Column('address', String))\n",
    "\n",
    "    # Create tables in the databases\n",
    "    metadata1.create_all(engine1)\n",
    "    metadata2.create_all(engine2)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Database setup completed for example1 and example2 with tables for employees, managers, and departments.\")\n",
    "\n",
    "    return engine1, engine2, metadata1, metadata2\n",
    "\n",
    "def main(debug=False):\n",
    "    setup_databases(debug=debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. graph_construction.py\n",
    "\n",
    "This module constructs the database schema graph based on the metadata and includes functions for adding metadata nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../../src/database_optimizer_networkx_rag_llama3/modules/graph_construction.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../../src/database_optimizer_networkx_rag_llama3/modules/graph_construction.py\n",
    "import networkx as nx\n",
    "\n",
    "def add_metadata_to_graph(metadata, db_name, graph, debug=False):\n",
    "    for table_name, table in metadata.tables.items():\n",
    "        table_node = f\"{db_name}.{table_name}\"\n",
    "        graph.add_node(table_node, type=\"table\", db=db_name, label=f\"Table: {table_name} ({db_name})\")\n",
    "\n",
    "        for column in table.columns:\n",
    "            column_node_id = f\"{db_name}.{table_name}.{column.name}\"\n",
    "            graph.add_node(column_node_id, type=\"column\", db=db_name, label=f\"Column: {column.name} ({table_name})\", data_type=str(column.type))\n",
    "            graph.add_edge(table_node, column_node_id, relationship=\"contains\")\n",
    "\n",
    "        for fk in table.foreign_keys:\n",
    "            parent_column = f\"{db_name}.{fk.parent.table.name}.{fk.parent.name}\"\n",
    "            referenced_column = f\"{db_name}.{fk.column.table.name}.{fk.column.name}\"\n",
    "            graph.add_edge(parent_column, referenced_column, relationship=\"foreign_key\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Metadata for {db_name} added to graph with {len(graph.nodes)} nodes and {len(graph.edges)} edges.\")\n",
    "\n",
    "def construct_graph(metadata1, metadata2, debug=False):\n",
    "    graph = nx.DiGraph()\n",
    "    add_metadata_to_graph(metadata1, \"example1\", graph, debug)\n",
    "    add_metadata_to_graph(metadata2, \"example2\", graph, debug)\n",
    "    return graph\n",
    "\n",
    "def main(debug=False):\n",
    "\n",
    "    _, _, metadata1, metadata2 = setup_databases(debug)\n",
    "    graph = construct_graph(metadata1, metadata2, debug=debug)\n",
    "    if debug:\n",
    "        print(\"Graph construction completed.\")\n",
    "    return graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. llm_analyzer.py\n",
    "\n",
    "This module defines the FlexibleDatabaseLLM class for querying the schema with customizable prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../../src/database_optimizer_networkx_rag_llama3/modules/llm_analyzer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../../src/database_optimizer_networkx_rag_llama3/modules/llm_analyzer.py\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "class FlexibleDatabaseLLM:\n",
    "    def __init__(self, graph, debug=False):\n",
    "        self.graph = graph\n",
    "        self.llm = ChatOllama(model=\"llama3.2\")\n",
    "        self.debug = debug\n",
    "\n",
    "    def extract_table_info(self):\n",
    "        table_info = {}\n",
    "        for node, attrs in self.graph.nodes(data=True):\n",
    "            if attrs['type'] == 'table':\n",
    "                table_info[node] = {'columns': [], 'db': attrs['db']}\n",
    "        for node, attrs in self.graph.nodes(data=True):\n",
    "            if attrs['type'] == 'column':\n",
    "                table_name = node.split('.')[0] + '.' + node.split('.')[1]\n",
    "                if table_name in table_info:\n",
    "                    table_info[table_name]['columns'].append(attrs['label'])\n",
    "        if self.debug:\n",
    "            print(\"Extracted table info:\", table_info)\n",
    "        return table_info\n",
    "\n",
    "    def query_schema_with_prompt(self, custom_prompt):\n",
    "        table_info = self.extract_table_info()\n",
    "        table_details = \"\\n\".join([f\"Table {key} has columns: {', '.join(value['columns'])}\" for key, value in table_info.items()])\n",
    "        prompt_content = f\"{custom_prompt}\\n\\n{table_details}\"\n",
    "        message = HumanMessage(content=prompt_content)\n",
    "        response = self.llm([message])\n",
    "        if self.debug:\n",
    "            print(f\"\\nLLM Analysis:\\n{response.content}\")\n",
    "        return response.content\n",
    "\n",
    "def main(debug=False):\n",
    "    _, _, metadata1, metadata2 = setup_databases(debug)\n",
    "    graph = construct_graph(metadata1, metadata2, debug=debug)\n",
    "    llm_analyzer = FlexibleDatabaseLLM(graph, debug=debug)\n",
    "    prompt = \"Identify any tables that appear to be duplicates or serve similar purposes.\"\n",
    "    response = llm_analyzer.query_schema_with_prompt(prompt)\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. visualization.py\n",
    "\n",
    "This module contains functions to visualize the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../../src/database_optimizer_networkx_rag_llama3/modules/visualization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../../src/database_optimizer_networkx_rag_llama3/modules/visualization.py\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_graph(graph, debug=False):\n",
    "    \"\"\"Generate a visual representation of the schema graph.\"\"\"\n",
    "    # Color nodes based on database source\n",
    "    node_colors = ['skyblue' if graph.nodes[n]['db'] == 'example1' else 'orange' for n in graph.nodes]\n",
    "    pos = nx.spring_layout(graph, k=0.5, iterations=50)\n",
    "    \n",
    "    # Create Matplotlib figure for Streamlit display\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    nx.draw(graph, pos, ax=ax, node_color=node_colors, with_labels=True, font_weight=\"bold\", font_size=8)\n",
    "    ax.set_title(\"Combined Database Schema Graph\", fontsize=18)\n",
    "\n",
    "    # Optionally display debug information\n",
    "    if debug:\n",
    "        print(f\"Graph has {len(graph.nodes)} nodes and {len(graph.edges)} edges.\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def main(debug=False):\n",
    "    _, _, metadata1, metadata2 = setup_databases(debug)\n",
    "    graph = construct_graph(metadata1, metadata2, debug=debug)\n",
    "    visualize_graph(graph, debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. main.py\n",
    "\n",
    "This file brings all modules together and runs the complete application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../../src/database_optimizer_networkx_rag_llama3/modules/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../../src/database_optimizer_networkx_rag_llama3/modules/main.py\n",
    "from database_setup import setup_databases\n",
    "from graph_construction import construct_graph, add_metadata_to_graph\n",
    "from llm_analyzer import FlexibleDatabaseLLM\n",
    "from visualization import visualize_graph\n",
    "import networkx as nx\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def process_uploaded_metadata(file_path, db_name, debug=False):\n",
    "    \"\"\"\n",
    "    Process metadata from uploaded files (JSON, CSV, or SQLAlchemy metadata).\n",
    "    Supports SQLAlchemy, JSON, and CSV formats to dynamically build the schema.\n",
    "    \"\"\"\n",
    "    graph = nx.DiGraph()\n",
    "    \n",
    "    if file_path.endswith('.json'):\n",
    "        with open(file_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        graph = parse_json_metadata(metadata, db_name, graph, debug)\n",
    "    \n",
    "    elif file_path.endswith('.csv'):\n",
    "        metadata = pd.read_csv(file_path)\n",
    "        graph = parse_csv_metadata(metadata, db_name, graph, debug)\n",
    "\n",
    "    elif isinstance(file_path, MetaData):  # For SQLAlchemy metadata\n",
    "        add_metadata_to_graph(file_path, db_name, graph, debug)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please upload JSON, CSV, or SQLAlchemy metadata.\")\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def parse_json_metadata(json_data, db_name, graph, debug=False):\n",
    "    \"\"\"\n",
    "    Parse JSON metadata to build nodes and edges in the graph.\n",
    "    JSON should include table and column definitions.\n",
    "    \"\"\"\n",
    "    for table, columns in json_data.items():\n",
    "        table_node = f\"{db_name}.{table}\"\n",
    "        graph.add_node(table_node, type=\"table\", db=db_name, label=f\"Table: {table} ({db_name})\")\n",
    "        \n",
    "        for column in columns:\n",
    "            column_node_id = f\"{db_name}.{table}.{column['name']}\"\n",
    "            graph.add_node(column_node_id, type=\"column\", db=db_name, label=f\"Column: {column['name']} ({table})\", data_type=column['type'])\n",
    "            graph.add_edge(table_node, column_node_id, relationship=\"contains\")\n",
    "\n",
    "        if debug:\n",
    "            print(f\"JSON metadata for {db_name} added to graph.\")\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def parse_csv_metadata(csv_data, db_name, graph, debug=False):\n",
    "    \"\"\"\n",
    "    Parse CSV metadata to build nodes and edges in the graph.\n",
    "    Assumes CSV has 'table', 'column', and 'type' columns.\n",
    "    \"\"\"\n",
    "    for _, row in csv_data.iterrows():\n",
    "        table_node = f\"{db_name}.{row['table']}\"\n",
    "        graph.add_node(table_node, type=\"table\", db=db_name, label=f\"Table: {row['table']} ({db_name})\")\n",
    "        \n",
    "        column_node_id = f\"{db_name}.{row['table']}.{row['column']}\"\n",
    "        graph.add_node(column_node_id, type=\"column\", db=db_name, label=f\"Column: {row['column']} ({row['table']})\", data_type=row['type'])\n",
    "        graph.add_edge(table_node, column_node_id, relationship=\"contains\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"CSV metadata for {db_name} added to graph.\")\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def main(file_path=None, file_type=\"sqlalchemy\", db_name=\"uploaded_db\", debug=False):\n",
    "    \"\"\"\n",
    "    Main function to automate database optimization:\n",
    "    - Allows input of various data formats (JSON, CSV, SQLAlchemy metadata)\n",
    "    - Constructs the database schema graph\n",
    "    - Visualizes it and performs schema analysis using LLM\n",
    "    \"\"\"\n",
    "    # Step 1: Setup initial example databases if no file is provided\n",
    "    if file_type == \"sqlalchemy\" and file_path is None:\n",
    "        engine1, engine2, metadata1, metadata2 = setup_databases(debug=debug)\n",
    "        graph = construct_graph(metadata1, metadata2, debug=debug)\n",
    "    else:\n",
    "        # Process uploaded metadata and create a new graph\n",
    "        graph = process_uploaded_metadata(file_path, db_name, debug=debug)\n",
    "    \n",
    "    # Step 2: Visualize the graph\n",
    "    visualize_graph(graph, debug=debug)\n",
    "    \n",
    "    # Step 3: LLM Analysis\n",
    "    llm_analyzer = FlexibleDatabaseLLM(graph, debug=debug)\n",
    "    prompt = \"Identify any tables that appear to be duplicates or serve similar purposes.\"\n",
    "    response = llm_analyzer.query_schema_with_prompt(prompt)\n",
    "    print(\"\\nLLM Analysis Result:\")\n",
    "    print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../../../src/database_optimizer_networkx_rag_llama3/streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../../../src/database_optimizer_networkx_rag_llama3/streamlit_app.py\n",
    "\n",
    "# streamlit_app.py\n",
    "\n",
    "import streamlit as st\n",
    "from modules.database_setup import setup_databases\n",
    "from modules.graph_construction import construct_graph, add_metadata_to_graph\n",
    "from modules.llm_analyzer import FlexibleDatabaseLLM\n",
    "from modules.visualization import visualize_graph\n",
    "import networkx as nx\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Title\n",
    "st.title(\"Database Schema Optimizer and Visualizer\")\n",
    "\n",
    "# Sidebar options\n",
    "st.sidebar.header(\"Options\")\n",
    "db_name = st.sidebar.text_input(\"Enter Database Name:\", \"uploaded_db\")\n",
    "debug = st.sidebar.checkbox(\"Enable Debug Mode\", value=False)\n",
    "\n",
    "# Initialize the example databases and load them\n",
    "def load_example_graph():\n",
    "    engine1, engine2, metadata1, metadata2 = setup_databases(debug=debug)\n",
    "    graph = construct_graph(metadata1, metadata2, debug=debug)\n",
    "    return graph\n",
    "\n",
    "# Placeholder for Graph, initially with example databases loaded\n",
    "graph = load_example_graph()\n",
    "\n",
    "# Display Example Databases Information\n",
    "st.sidebar.header(\"Example Databases Loaded\")\n",
    "st.sidebar.write(\"The example databases are loaded by default. Upload new metadata to overwrite.\")\n",
    "\n",
    "# Metadata Upload Section\n",
    "st.sidebar.header(\"Upload Your Database Metadata\")\n",
    "file_upload = st.sidebar.file_uploader(\"Upload Metadata (JSON, CSV)\", type=[\"json\", \"csv\"])\n",
    "\n",
    "# Load and process uploaded metadata if present\n",
    "def parse_json_metadata(json_data, db_name, graph, debug=False):\n",
    "    \"\"\"Parse JSON metadata to add nodes and edges to the graph.\"\"\"\n",
    "    for table, columns in json_data.items():\n",
    "        table_node = f\"{db_name}.{table}\"\n",
    "        graph.add_node(table_node, type=\"table\", db=db_name, label=f\"Table: {table} ({db_name})\")\n",
    "        for column in columns:\n",
    "            column_node_id = f\"{db_name}.{table}.{column['name']}\"\n",
    "            graph.add_node(column_node_id, type=\"column\", db=db_name, label=f\"Column: {column['name']} ({table})\", data_type=column['type'])\n",
    "            graph.add_edge(table_node, column_node_id, relationship=\"contains\")\n",
    "    if debug:\n",
    "        st.sidebar.write(f\"JSON metadata for {db_name} processed.\")\n",
    "    return graph\n",
    "\n",
    "def parse_csv_metadata(csv_data, db_name, graph, debug=False):\n",
    "    \"\"\"Parse CSV metadata to add nodes and edges to the graph.\"\"\"\n",
    "    for _, row in csv_data.iterrows():\n",
    "        table_node = f\"{db_name}.{row['table']}\"\n",
    "        graph.add_node(table_node, type=\"table\", db=db_name, label=f\"Table: {row['table']} ({db_name})\")\n",
    "        column_node_id = f\"{db_name}.{row['table']}.{row['column']}\"\n",
    "        graph.add_node(column_node_id, type=\"column\", db=db_name, label=f\"Column: {row['column']} ({row['table']})\", data_type=row['type'])\n",
    "        graph.add_edge(table_node, column_node_id, relationship=\"contains\")\n",
    "    if debug:\n",
    "        st.sidebar.write(f\"CSV metadata for {db_name} processed.\")\n",
    "    return graph\n",
    "\n",
    "# Process uploaded metadata and replace graph if uploaded\n",
    "if file_upload:\n",
    "    file_type = file_upload.name.split(\".\")[-1]\n",
    "    graph = nx.DiGraph()  # Reset graph for new metadata\n",
    "    if file_type == \"json\":\n",
    "        metadata = json.load(file_upload)\n",
    "        graph = parse_json_metadata(metadata, db_name, graph, debug=debug)\n",
    "        st.sidebar.success(\"JSON Metadata Processed Successfully.\")\n",
    "    elif file_type == \"csv\":\n",
    "        metadata = pd.read_csv(file_upload)\n",
    "        graph = parse_csv_metadata(metadata, db_name, graph, debug=debug)\n",
    "        st.sidebar.success(\"CSV Metadata Processed Successfully.\")\n",
    "else:\n",
    "    st.sidebar.info(\"Default example databases are loaded.\")\n",
    "\n",
    "# Instructions for connecting to external databases\n",
    "st.sidebar.header(\"Instructions for External Databases\")\n",
    "st.sidebar.write(\"To use other databases (e.g., Oracle, Snowflake), extract metadata and save as JSON or CSV for upload.\")\n",
    "\n",
    "st.sidebar.code(\"\"\"\n",
    "# Example: Extracting Oracle metadata\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "engine = create_engine('oracle://user:password@host:port/dbname')\n",
    "metadata = MetaData()\n",
    "metadata.reflect(bind=engine)\n",
    "\"\"\", language=\"python\")\n",
    "\n",
    "# Graph Visualization\n",
    "st.subheader(\"Schema Graph Visualization\")\n",
    "fig = visualize_graph(graph, debug=debug)\n",
    "st.pyplot(fig)  # Display graph in Streamlit\n",
    "\n",
    "# LLM Schema Analysis\n",
    "st.sidebar.header(\"Database SME and \")\n",
    "custom_prompt = st.sidebar.text_area(\"Enter Analysis Prompt\", \"Identify any tables that appear to be duplicates or serve similar purposes.\")\n",
    "if st.sidebar.button(\"Run Analysis\"):\n",
    "    llm_analyzer = FlexibleDatabaseLLM(graph, debug=debug)\n",
    "    response = llm_analyzer.query_schema_with_prompt(custom_prompt)\n",
    "    st.subheader(\"LLM Analysis Result\")\n",
    "    st.write(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
